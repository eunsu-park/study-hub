"""
Dagster-dbt Integration Example
================================
Demonstrates how to load a dbt project as Dagster assets, creating a
unified lineage graph that spans both SQL (dbt) and Python transformations.

This example shows:
- Loading dbt models as Dagster assets via @dbt_assets
- Materializing dbt models through Dagster's orchestration
- Combining dbt SQL models with Python-based assets in one graph
- Configuring the DbtCliResource for different environments

Requirements:
    pip install dagster dagster-dbt dbt-core dbt-duckdb pandas

Usage:
    # Ensure your dbt project is compiled first:
    # cd /path/to/dbt_project && dbt compile

    # Then launch Dagster:
    dagster dev -f dagster_dbt.py

Note:
    This file is a self-contained illustration. In a real project,
    the dbt project would be in a separate directory and the manifest
    would be generated by `dbt compile` or `dbt build`.
"""

import dagster as dg
import pandas as pd
from pathlib import Path
from dataclasses import dataclass


# =============================================================================
# dbt Project Configuration
# =============================================================================

# Why define the project path as a constant?
# - Referenced in multiple places (resource config, @dbt_assets decorator)
# - Easy to override for testing (point to a test dbt project)
# - Follows Dagster's convention of centralizing configuration

DBT_PROJECT_DIR = Path(__file__).parent / "sample_dbt_project"

# In a real project, this manifest is generated by `dbt compile` or `dbt build`.
# It contains the full DAG of dbt models, sources, tests, and exposures.
DBT_MANIFEST_PATH = DBT_PROJECT_DIR / "target" / "manifest.json"


# =============================================================================
# dbt Resource Configuration
# =============================================================================

# Why use DbtCliResource?
# - Wraps the dbt CLI with proper environment handling
# - Streams dbt output as Dagster events (logs, test results, metadata)
# - Handles dbt project directory, profiles, and target configuration
# - Supports both local and cloud dbt execution

# NOTE: In production code, uncomment the import and use the real resource:
# from dagster_dbt import DbtCliResource, dbt_assets


@dataclass
class MockDbtCliResource:
    """Mock dbt CLI resource for demonstration when dagster-dbt is not installed.

    In production, replace this with:
        from dagster_dbt import DbtCliResource

    The real DbtCliResource:
    - Runs dbt CLI commands (build, run, test, compile)
    - Parses dbt output into Dagster events
    - Maps dbt models to Dagster assets automatically
    """
    project_dir: str = ""
    target: str = "dev"
    profiles_dir: str = ""

    def cli(self, args: list[str], context=None):
        """Execute a dbt CLI command (mock implementation)."""
        print(f"[Mock dbt] Running: dbt {' '.join(args)}")
        print(f"[Mock dbt] Project: {self.project_dir}")
        print(f"[Mock dbt] Target: {self.target}")
        return MockDbtCliOutput(args=args)


@dataclass
class MockDbtCliOutput:
    """Mock output from dbt CLI execution."""
    args: list[str]

    def stream(self):
        """Stream dbt events as Dagster materializations (mock)."""
        model_names = ["stg_orders", "stg_customers", "stg_products",
                       "int_order_items", "fct_orders", "dim_customers"]
        for model in model_names:
            print(f"[Mock dbt] Materialized: {model}")
            yield dg.AssetMaterialization(
                asset_key=dg.AssetKey(["dbt", model]),
                metadata={"model": model, "status": "success"},
            )

    def wait_for_completion(self):
        """Wait for dbt to finish (mock: instant)."""
        return self


# =============================================================================
# dbt Assets — Each dbt Model Becomes a Dagster Asset
# =============================================================================

# Why use @dbt_assets?
# - Automatically discovers ALL models in your dbt project
# - Each model becomes a Dagster asset with proper dependencies
# - dbt tests become Dagster asset checks
# - Unified lineage: SQL (dbt) and Python assets in one graph

# NOTE: In production with a real dbt project and dagster-dbt installed:
#
# @dbt_assets(manifest=DBT_MANIFEST_PATH)
# def my_dbt_assets(context: dg.AssetExecutionContext, dbt: DbtCliResource):
#     """All dbt models as Dagster assets.
#
#     When this is materialized, Dagster runs `dbt build` and maps
#     each model's result to the corresponding Dagster asset.
#     """
#     yield from dbt.cli(["build"], context=context).stream()

# For this example, we simulate the dbt assets with pure Python:

@dg.asset(
    key=dg.AssetKey(["dbt", "stg_orders"]),
    group_name="dbt_staging",
    description="[dbt] Staging model: cleaned orders from source",
    compute_kind="dbt",
    metadata={"dbt_model": "stg_orders", "materialization": "table"},
)
def stg_orders(context: dg.AssetExecutionContext) -> pd.DataFrame:
    """Simulates a dbt staging model for orders.

    In a real setup, this would be a SQL model like:

    -- models/staging/stg_orders.sql
    SELECT
        order_id,
        customer_id,
        order_date,
        total_amount,
        status,
        CURRENT_TIMESTAMP AS _loaded_at
    FROM {{ source('raw', 'orders') }}
    WHERE order_id IS NOT NULL
    """
    context.log.info("[dbt simulation] Building stg_orders")
    return pd.DataFrame({
        "order_id": range(1, 101),
        "customer_id": [i % 20 + 1 for i in range(100)],
        "order_date": pd.date_range("2024-01-01", periods=100, freq="6h"),
        "total_amount": [round(50 + i * 3.5, 2) for i in range(100)],
        "status": ["completed"] * 80 + ["pending"] * 10 + ["refunded"] * 10,
    })


@dg.asset(
    key=dg.AssetKey(["dbt", "stg_customers"]),
    group_name="dbt_staging",
    description="[dbt] Staging model: customer profiles from source",
    compute_kind="dbt",
    metadata={"dbt_model": "stg_customers", "materialization": "table"},
)
def stg_customers(context: dg.AssetExecutionContext) -> pd.DataFrame:
    """Simulates a dbt staging model for customers.

    -- models/staging/stg_customers.sql
    SELECT
        customer_id,
        customer_name,
        email,
        signup_date,
        country
    FROM {{ source('raw', 'customers') }}
    """
    context.log.info("[dbt simulation] Building stg_customers")
    countries = ["US", "UK", "DE", "FR", "JP"]
    return pd.DataFrame({
        "customer_id": range(1, 21),
        "customer_name": [f"Customer_{i}" for i in range(1, 21)],
        "email": [f"customer{i}@example.com" for i in range(1, 21)],
        "signup_date": pd.date_range("2023-01-01", periods=20, freq="18D"),
        "country": [countries[i % len(countries)] for i in range(20)],
    })


@dg.asset(
    key=dg.AssetKey(["dbt", "fct_orders"]),
    group_name="dbt_marts",
    description="[dbt] Fact table: orders enriched with customer data",
    compute_kind="dbt",
    deps=[dg.AssetKey(["dbt", "stg_orders"]), dg.AssetKey(["dbt", "stg_customers"])],
)
def fct_orders(
    context: dg.AssetExecutionContext,
    stg_orders: pd.DataFrame,
    stg_customers: pd.DataFrame,
) -> pd.DataFrame:
    """Simulates a dbt mart/fact model.

    -- models/marts/fct_orders.sql
    SELECT
        o.order_id,
        o.customer_id,
        c.customer_name,
        c.country,
        o.order_date,
        o.total_amount,
        o.status
    FROM {{ ref('stg_orders') }} o
    LEFT JOIN {{ ref('stg_customers') }} c
        ON o.customer_id = c.customer_id
    WHERE o.status = 'completed'

    Why a fact table?
      - Central to the star schema: one row per business event (order)
      - Enriched with dimension attributes (customer name, country)
      - Filtered to completed orders (business definition of "revenue")
    """
    context.log.info("[dbt simulation] Building fct_orders")
    df = stg_orders.merge(stg_customers, on="customer_id", how="left")
    df = df[df["status"] == "completed"]
    context.log.info(f"fct_orders: {len(df)} completed orders")
    return df


# =============================================================================
# Python Assets — Extending dbt with Python Logic
# =============================================================================

# Why mix dbt and Python?
# - dbt excels at SQL transformations: joins, aggregations, window functions
# - Python excels at: ML features, API calls, complex business logic, NLP
# - Dagster unifies both in a single dependency graph and UI

@dg.asset(
    group_name="ml_features",
    description="ML features derived from dbt fact tables (Python-computed)",
    compute_kind="pandas",
    deps=[dg.AssetKey(["dbt", "fct_orders"])],
)
def customer_ml_features(
    context: dg.AssetExecutionContext,
    fct_orders: pd.DataFrame,
) -> pd.DataFrame:
    """Compute ML features from dbt-transformed data.

    This asset depends on a dbt model (fct_orders), demonstrating
    how Python and dbt assets coexist in the same dependency graph.

    Why Python for ML features?
      - Feature engineering often involves custom logic (RFM scoring, embeddings)
      - Python has rich ML libraries (scikit-learn, numpy)
      - Some features need statistical computations hard to express in SQL
    """
    context.log.info("Computing ML features from fct_orders")

    features = fct_orders.groupby("customer_id").agg(
        # Frequency: how many orders
        order_count=("order_id", "count"),
        # Monetary: total and average spend
        total_spend=("total_amount", "sum"),
        avg_order_value=("total_amount", "mean"),
        # Recency: days since last order (simplified)
        last_order_date=("order_date", "max"),
        first_order_date=("order_date", "min"),
        # Geographic feature
        country=("country", "first"),
    ).reset_index()

    # Derived features
    # Why log-transform monetary features?
    # - Spend distributions are typically right-skewed
    # - Log transform makes them more Gaussian → better for many ML models
    import numpy as np
    features["log_total_spend"] = np.log1p(features["total_spend"])

    # Customer tenure in days
    features["tenure_days"] = (
        features["last_order_date"] - features["first_order_date"]
    ).dt.days

    # Average days between orders (purchase frequency)
    features["avg_days_between_orders"] = (
        features["tenure_days"] / features["order_count"].clip(lower=1)
    ).round(1)

    context.log.info(f"Computed {len(features.columns)} features for {len(features)} customers")
    return features


@dg.asset(
    group_name="ml_features",
    description="Customer churn risk scores (ML model predictions)",
    compute_kind="python",
)
def churn_risk_scores(
    context: dg.AssetExecutionContext,
    customer_ml_features: pd.DataFrame,
) -> pd.DataFrame:
    """Score customers with a churn prediction model.

    In production, this would load a trained model from MLflow/S3.
    Here we use a simple heuristic to demonstrate the pattern.

    Why as a Dagster asset?
      - Dagster tracks when scores were last updated
      - Downstream dashboards/alerts depend on fresh scores
      - If the model or features change, Dagster knows to re-score
    """
    context.log.info("Scoring churn risk for customers")

    scores = customer_ml_features[["customer_id"]].copy()

    # Simple heuristic (replace with actual model in production)
    # Higher risk if: low order count, low tenure, high avg days between orders
    risk_score = (
        (1 / customer_ml_features["order_count"].clip(lower=1)) * 30 +
        (1 / customer_ml_features["tenure_days"].clip(lower=1)) * 20 +
        (customer_ml_features["avg_days_between_orders"] / 30) * 50
    ).clip(0, 100).round(2)

    scores["churn_risk_score"] = risk_score
    scores["risk_tier"] = pd.cut(
        risk_score,
        bins=[0, 25, 50, 75, 100],
        labels=["low", "medium", "high", "critical"],
    )
    scores["scored_at"] = pd.Timestamp.now()

    context.log.info(
        f"Churn scores: {scores['risk_tier'].value_counts().to_dict()}"
    )
    return scores


# =============================================================================
# Definitions — Unified dbt + Python Pipeline
# =============================================================================

defs = dg.Definitions(
    assets=[
        # dbt models (staging + marts)
        stg_orders,
        stg_customers,
        fct_orders,
        # Python assets (ML features)
        customer_ml_features,
        churn_risk_scores,
    ],
    resources={
        # Why configure dbt resource here?
        # - Different project paths for dev vs prod
        # - Different dbt profiles (dev → DuckDB, prod → BigQuery)
        "dbt": MockDbtCliResource(
            project_dir=str(DBT_PROJECT_DIR),
            target="dev",
        ),
        "io_manager": dg.mem_io_manager,
    },
)


# =============================================================================
# Entry Point — Test the full dbt + Python pipeline
# =============================================================================

if __name__ == "__main__":
    print("=" * 60)
    print("Dagster-dbt Integration Example")
    print("=" * 60)
    print("\nMaterializing the full dbt + Python asset graph...\n")

    result = dg.materialize(
        assets=[stg_orders, stg_customers, fct_orders,
                customer_ml_features, churn_risk_scores],
        resources={"io_manager": dg.mem_io_manager},
    )

    if result.success:
        print("\nAll assets materialized successfully!")

        features = result.output_for_node("customer_ml_features")
        print(f"\nML Features (first 5 customers):")
        print(features.head().to_string())

        scores = result.output_for_node("churn_risk_scores")
        print(f"\nChurn Risk Scores (first 5):")
        print(scores.head().to_string())

        print(f"\nAsset Graph (execution order):")
        print("  stg_orders ──┐")
        print("               ├──→ fct_orders ──→ customer_ml_features ──→ churn_risk_scores")
        print("  stg_customers┘")
    else:
        print("Pipeline FAILED!")
