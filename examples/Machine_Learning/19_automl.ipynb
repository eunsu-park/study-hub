{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML and Hyperparameter Optimization\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Optuna for hyperparameter optimization\n",
    "- Pruning (early stopping of bad trials)\n",
    "- Multi-objective optimization\n",
    "- Comparing HPO strategies\n",
    "\n",
    "**Requirements**: `pip install optuna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "X, y = housing.data, housing.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f'Train: {X_train.shape}, Test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: Optuna uses Bayesian optimization (Tree-structured Parzen Estimator) to sample\n# promising hyperparameter regions, converging much faster than grid search. suggest_float\n# with log=True samples learning_rate on a log scale because its effect on performance\n# is multiplicative, not additive.\ndef objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n        'max_depth': trial.suggest_int('max_depth', 2, 10),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n    }\n    model = GradientBoostingRegressor(**params, random_state=42)\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n    return -scores.mean()\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50, show_progress_bar=True)\n\nprint(f'Best MSE: {study.best_value:.4f}')\nprint(f'Best params: {study.best_params}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig = optuna.visualization.plot_optimization_history(study)\n",
    "fig.show()\n",
    "\n",
    "fig = optuna.visualization.plot_param_importances(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: Comparing the optimized model against the default model quantifies the actual\n# benefit of HPO — if the improvement is small, the default hyperparameters may be\n# sufficient and the optimization effort is not worthwhile for this dataset.\nbest = GradientBoostingRegressor(**study.best_params, random_state=42)\nbest.fit(X_train, y_train)\ny_pred = best.predict(X_test)\n\n# Compare with default\ndefault = GradientBoostingRegressor(random_state=42)\ndefault.fit(X_train, y_train)\ny_pred_def = default.predict(X_test)\n\nprint(f'Default   - R²: {r2_score(y_test, y_pred_def):.4f}, RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_def)):.4f}')\nprint(f'Optimized - R²: {r2_score(y_test, y_pred):.4f}, RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Objective Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: Multi-objective optimization finds Pareto-optimal trade-offs between accuracy\n# and model complexity — in production, a slightly less accurate but 10x simpler model\n# may be preferred for faster inference and easier maintenance.\ndef multi_obj(trial):\n    n_est = trial.suggest_int('n_estimators', 10, 500)\n    depth = trial.suggest_int('max_depth', 2, 10)\n    lr = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n    \n    model = GradientBoostingRegressor(n_estimators=n_est, max_depth=depth, learning_rate=lr, random_state=42)\n    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='neg_mean_squared_error')\n    mse = -scores.mean()\n    # Why: n_estimators * 2^max_depth approximates total leaf nodes, serving as a proxy\n    # for model complexity (memory footprint and inference latency).\n    complexity = n_est * (2 ** depth)\n    return mse, complexity\n\nmo_study = optuna.create_study(directions=['minimize', 'minimize'])\nmo_study.optimize(multi_obj, n_trials=50, show_progress_bar=True)\n\n# Pareto front\npareto = mo_study.best_trials\nprint(f'Pareto solutions: {len(pareto)}')\nfor t in pareto[:5]:\n    print(f'  MSE={t.values[0]:.4f}, Complexity={t.values[1]:.0f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Pareto front\n",
    "all_mse = [t.values[0] for t in mo_study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "all_cplx = [t.values[1] for t in mo_study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "pareto_mse = [t.values[0] for t in pareto]\n",
    "pareto_cplx = [t.values[1] for t in pareto]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(all_mse, all_cplx, alpha=0.4, label='All trials')\n",
    "plt.scatter(pareto_mse, pareto_cplx, c='red', s=80, marker='*', label='Pareto front')\n",
    "plt.xlabel('MSE (lower=better)'); plt.ylabel('Complexity (lower=simpler)')\n",
    "plt.title('Multi-Objective: Accuracy vs Complexity')\n",
    "plt.legend(); plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}