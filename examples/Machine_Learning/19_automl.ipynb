{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML and Hyperparameter Optimization\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Optuna for hyperparameter optimization\n",
    "- Pruning (early stopping of bad trials)\n",
    "- Multi-objective optimization\n",
    "- Comparing HPO strategies\n",
    "\n",
    "**Requirements**: `pip install optuna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "X, y = housing.data, housing.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f'Train: {X_train.shape}, Test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "    }\n",
    "    model = GradientBoostingRegressor(**params, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    return -scores.mean()\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f'Best MSE: {study.best_value:.4f}')\n",
    "print(f'Best params: {study.best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig = optuna.visualization.plot_optimization_history(study)\n",
    "fig.show()\n",
    "\n",
    "fig = optuna.visualization.plot_param_importances(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best model\n",
    "best = GradientBoostingRegressor(**study.best_params, random_state=42)\n",
    "best.fit(X_train, y_train)\n",
    "y_pred = best.predict(X_test)\n",
    "\n",
    "# Compare with default\n",
    "default = GradientBoostingRegressor(random_state=42)\n",
    "default.fit(X_train, y_train)\n",
    "y_pred_def = default.predict(X_test)\n",
    "\n",
    "print(f'Default   - R²: {r2_score(y_test, y_pred_def):.4f}, RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_def)):.4f}')\n",
    "print(f'Optimized - R²: {r2_score(y_test, y_pred):.4f}, RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Objective Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_obj(trial):\n",
    "    n_est = trial.suggest_int('n_estimators', 10, 500)\n",
    "    depth = trial.suggest_int('max_depth', 2, 10)\n",
    "    lr = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "    \n",
    "    model = GradientBoostingRegressor(n_estimators=n_est, max_depth=depth, learning_rate=lr, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='neg_mean_squared_error')\n",
    "    mse = -scores.mean()\n",
    "    complexity = n_est * (2 ** depth)\n",
    "    return mse, complexity\n",
    "\n",
    "mo_study = optuna.create_study(directions=['minimize', 'minimize'])\n",
    "mo_study.optimize(multi_obj, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Pareto front\n",
    "pareto = mo_study.best_trials\n",
    "print(f'Pareto solutions: {len(pareto)}')\n",
    "for t in pareto[:5]:\n",
    "    print(f'  MSE={t.values[0]:.4f}, Complexity={t.values[1]:.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Pareto front\n",
    "all_mse = [t.values[0] for t in mo_study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "all_cplx = [t.values[1] for t in mo_study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "pareto_mse = [t.values[0] for t in pareto]\n",
    "pareto_cplx = [t.values[1] for t in pareto]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(all_mse, all_cplx, alpha=0.4, label='All trials')\n",
    "plt.scatter(pareto_mse, pareto_cplx, c='red', s=80, marker='*', label='Pareto front')\n",
    "plt.xlabel('MSE (lower=better)'); plt.ylabel('Complexity (lower=simpler)')\n",
    "plt.title('Multi-Objective: Accuracy vs Complexity')\n",
    "plt.legend(); plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.9.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
