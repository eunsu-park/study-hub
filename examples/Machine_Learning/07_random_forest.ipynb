{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. 랜덤 포레스트 (Random Forest)\n",
    "\n",
    "## 학습 목표\n",
    "- 앙상블 학습과 배깅 이해\n",
    "- 랜덤 포레스트 작동 원리\n",
    "- 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.datasets import load_iris, load_wine, fetch_california_housing\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 랜덤 포레스트 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine 데이터셋 로드\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "print(f\"Features: {wine.feature_names}\")\n",
    "print(f\"Classes: {wine.target_names}\")\n",
    "print(f\"Shape: {X.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: n_estimators=100 trees reduces variance through averaging (bagging).\n# max_depth=5 prevents individual trees from overfitting.\n# n_jobs=-1 parallelizes tree training across all CPU cores — RF trees are\n# independent, making it embarrassingly parallel.\nrf_clf = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=5,\n    random_state=42,\n    n_jobs=-1\n)\nrf_clf.fit(X_train, y_train)\n\ny_pred = rf_clf.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=wine.target_names))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 트리 개수에 따른 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: oob_score=True uses out-of-bag samples (data not in each tree's bootstrap)\n# as a free validation set. This gives an unbiased estimate of generalization\n# without needing a separate validation split — unique advantage of bagging.\nn_trees = [1, 5, 10, 20, 50, 100, 200, 500]\ntrain_scores = []\ntest_scores = []\noob_scores = []\n\nfor n in n_trees:\n    rf = RandomForestClassifier(n_estimators=n, random_state=42, oob_score=True)\n    rf.fit(X_train, y_train)\n    train_scores.append(rf.score(X_train, y_train))\n    test_scores.append(rf.score(X_test, y_test))\n    oob_scores.append(rf.oob_score_)\n\nplt.figure(figsize=(10, 6))\nplt.plot(n_trees, train_scores, 'b-o', label='Train Score')\nplt.plot(n_trees, test_scores, 'r-o', label='Test Score')\nplt.plot(n_trees, oob_scores, 'g-o', label='OOB Score')\nplt.xlabel('Number of Trees')\nplt.ylabel('Accuracy')\nplt.title('Random Forest: Performance vs Number of Trees')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xscale('log')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 특성 중요도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성 중요도\n",
    "importance = pd.DataFrame({\n",
    "    'Feature': wine.feature_names,\n",
    "    'Importance': rf_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(importance['Feature'], importance['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest Feature Importance - Wine Dataset')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: max_features controls how many features each tree considers at each split.\n# 'sqrt' (default) decorrelates trees — if all trees use the same dominant feature,\n# averaging provides little variance reduction. This is RF's key advantage over bagging.\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [3, 5, 7, None],\n    'min_samples_split': [2, 5, 10],\n    'max_features': ['sqrt', 'log2', None]\n}\n\ngrid_search = GridSearchCV(\n    RandomForestClassifier(random_state=42, n_jobs=-1),\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(f\"Best Parameters: {grid_search.best_params_}\")\nprint(f\"Best CV Score: {grid_search.best_score_:.4f}\")\nprint(f\"Test Score: {grid_search.score(X_test, y_test):.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 결정 트리 vs 랜덤 포레스트 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.tree import DecisionTreeClassifier\n\n# Why: This comparison shows how ensembling reduces variance. A single tree has\n# high variance (different splits on different data), but averaging 100 trees\n# cancels out individual errors, yielding a more stable and accurate model.\ndt = DecisionTreeClassifier(max_depth=5, random_state=42)\nrf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n\ndt_scores = cross_val_score(dt, X, y, cv=10)\nrf_scores = cross_val_score(rf, X, y, cv=10)\n\nprint(f\"Decision Tree: {dt_scores.mean():.4f} (+/- {dt_scores.std()*2:.4f})\")\nprint(f\"Random Forest: {rf_scores.mean():.4f} (+/- {rf_scores.std()*2:.4f})\")\n\n# 박스플롯 비교\nplt.figure(figsize=(8, 6))\nplt.boxplot([dt_scores, rf_scores], labels=['Decision Tree', 'Random Forest'])\nplt.ylabel('Accuracy')\nplt.title('Model Comparison: Decision Tree vs Random Forest')\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 랜덤 포레스트 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# California Housing 데이터\n",
    "housing = fetch_california_housing()\n",
    "X_h, y_h = housing.data, housing.target\n",
    "\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
    "    X_h, y_h, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 랜덤 포레스트 회귀\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_reg.fit(X_train_h, y_train_h)\n",
    "\n",
    "y_pred_h = rf_reg.predict(X_test_h)\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "print(f\"R² Score: {r2_score(y_test_h, y_pred_h):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test_h, y_pred_h)):.4f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test_h, y_pred_h):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성 중요도 (회귀)\n",
    "importance_reg = pd.DataFrame({\n",
    "    'Feature': housing.feature_names,\n",
    "    'Importance': rf_reg.feature_importances_\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_reg['Feature'], importance_reg['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest Regressor Feature Importance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "### 핵심 개념\n",
    "- **배깅 (Bagging)**: Bootstrap Aggregating, 여러 모델의 예측을 평균/투표\n",
    "- **랜덤 특성 선택**: 각 분할에서 일부 특성만 고려\n",
    "- **OOB (Out-of-Bag) Score**: 부트스트랩에 포함되지 않은 샘플로 평가\n",
    "\n",
    "### 주요 하이퍼파라미터\n",
    "- `n_estimators`: 트리 개수 (많을수록 좋지만 수익 체감)\n",
    "- `max_depth`: 트리 깊이 (과적합 방지)\n",
    "- `max_features`: 분할 시 고려할 특성 수\n",
    "- `min_samples_split`: 분할을 위한 최소 샘플 수\n",
    "\n",
    "### 다음 단계\n",
    "- Gradient Boosting (XGBoost, LightGBM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}