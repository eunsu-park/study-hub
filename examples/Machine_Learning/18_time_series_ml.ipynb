{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Machine Learning\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Converting time series to supervised learning\n",
    "- Feature engineering (lags, rolling, calendar, Fourier)\n",
    "- TimeSeriesSplit cross-validation\n",
    "- Tree-based forecasting (GradientBoosting)\n",
    "- Walk-forward backtesting\n",
    "- Time series classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, classification_report\n",
    "\n",
    "np.random.seed(42)\n",
    "print('Libraries loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 730\n",
    "dates = pd.date_range('2022-01-01', periods=n_days, freq='D')\n",
    "trend = np.linspace(100, 150, n_days)\n",
    "weekly = 15 * np.sin(2 * np.pi * np.arange(n_days) / 7)\n",
    "yearly = 30 * np.sin(2 * np.pi * np.arange(n_days) / 365.25)\n",
    "noise = np.random.normal(0, 5, n_days)\n",
    "sales = trend + weekly + yearly + noise\n",
    "\n",
    "df = pd.DataFrame({'date': dates, 'sales': sales})\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(df['date'], df['sales'], linewidth=0.8)\n",
    "plt.title('Daily Sales (2 years)')\n",
    "plt.xlabel('Date'); plt.ylabel('Sales')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Lag features\nfor lag in [1, 7, 14, 28, 365]:\n    df[f'lag_{lag}'] = df['sales'].shift(lag)\n\n# Why: shift(1) before rolling prevents target leakage — each row's rolling statistic\n# must use only past data, not include the current day's actual sales value.\nfor w in [7, 14, 30]:\n    shifted = df['sales'].shift(1)\n    df[f'roll_mean_{w}'] = shifted.rolling(w).mean()\n    df[f'roll_std_{w}'] = shifted.rolling(w).std()\n\n# Calendar features\ndf['dow'] = df['date'].dt.dayofweek\ndf['month'] = df['date'].dt.month\ndf['is_weekend'] = df['date'].dt.dayofweek.isin([5, 6]).astype(int)\n\n# Why: Cyclical sin/cos encoding ensures the model treats day 6 (Sunday) and day 0\n# (Monday) as neighbors, and December and January as adjacent months.\ndf['dow_sin'] = np.sin(2 * np.pi * df['dow'] / 7)\ndf['dow_cos'] = np.cos(2 * np.pi * df['dow'] / 7)\ndf['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\ndf['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n\n# Why: Multiple Fourier harmonics (k=1,2,3) capture both the fundamental yearly cycle\n# and its sharper sub-patterns (e.g., holiday season peaks), providing richer seasonal\n# representation than a single sine wave.\nt = np.arange(len(df))\nfor k in range(1, 4):\n    df[f'year_sin_{k}'] = np.sin(2 * np.pi * k * t / 365.25)\n    df[f'year_cos_{k}'] = np.cos(2 * np.pi * k * t / 365.25)\n\nprint(f'Features created: {len(df.columns) - 2}')\ndf.dropna(inplace=True)\ndf.reset_index(drop=True, inplace=True)\nprint(f'Rows after dropping NaN: {len(df)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TimeSeriesSplit Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "feat_cols = [c for c in df.columns if c not in ['date', 'sales']]\nX = df[feat_cols]\ny = df['sales']\n\n# Why: TimeSeriesSplit respects temporal ordering — each fold uses only past data for\n# training and future data for validation. Standard KFold would leak future information\n# into training, producing overly optimistic scores that don't reflect real forecasting.\ntscv = TimeSeriesSplit(n_splits=5)\ngb = GradientBoostingRegressor(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)\nscores = cross_val_score(gb, X, y, cv=tscv, scoring='neg_mean_absolute_error')\nprint(f'TimeSeriesSplit MAE: {-scores.mean():.2f} (+/- {scores.std():.2f})')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = len(X) - 60\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "dates_test = df['date'].iloc[split:]\n",
    "\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.1f}%')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.plot(dates_test, y_test, 'b-', label='Actual', lw=2)\n",
    "ax.plot(dates_test, y_pred, 'r--', label='Predicted', lw=2)\n",
    "ax.set_title(f'GB Forecast (MAE={mae:.2f})')\n",
    "ax.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "imp = pd.Series(gb.feature_importances_, index=feat_cols).nlargest(15)\n",
    "imp.sort_values().plot(kind='barh', figsize=(10, 6), color='steelblue')\n",
    "plt.title('Top 15 Features'); plt.xlabel('Importance')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Time Series Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.model_selection import train_test_split\n\nn_series, length = 300, 100\nX_ts, y_ts = [], []\nfor _ in range(n_series):\n    label = np.random.choice([0, 1, 2])\n    t = np.arange(length)\n    if label == 0: s = np.cumsum(np.random.normal(0, 1, length))\n    elif label == 1: s = 0.5 * t + np.random.normal(0, 3, length)\n    else: s = 10 * np.sin(2 * np.pi * t / 20) + np.random.normal(0, 1, length)\n    X_ts.append(s); y_ts.append(label)\n\nX_ts = np.array(X_ts); y_ts = np.array(y_ts)\n\n# Why: Hand-crafted statistical features (trend slope, autocorrelation, zero-crossing\n# rate, skewness, kurtosis) convert variable-length time series into a fixed-size\n# feature vector that standard classifiers can consume — this is the \"feature-based\"\n# approach to time series classification.\nfeats = pd.DataFrame([{\n    'mean': s.mean(), 'std': s.std(), 'trend': np.polyfit(range(len(s)), s, 1)[0],\n    'autocorr': pd.Series(s).autocorr(1), 'crossing': np.mean(np.diff(np.sign(s - s.mean())) != 0),\n    'skew': pd.Series(s).skew(), 'kurtosis': pd.Series(s).kurtosis()\n} for s in X_ts])\n\nXtr, Xte, ytr, yte = train_test_split(feats, y_ts, test_size=0.3, random_state=42)\nclf = RandomForestClassifier(100, random_state=42)\nclf.fit(Xtr, ytr)\nprint(classification_report(yte, clf.predict(Xte), target_names=['Stationary','Trending','Seasonal']))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}