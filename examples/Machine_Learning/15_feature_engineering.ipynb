{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "This notebook demonstrates key feature engineering techniques:\n",
    "- Numerical transformations (scaling, binning, polynomial features)\n",
    "- Categorical encoding (target, frequency, hash encoding)\n",
    "- Date/time features (cyclical encoding, lag features)\n",
    "- Text features (TF-IDF, basic statistics)\n",
    "- Feature selection (filter, wrapper, embedded methods)\n",
    "- End-to-end pipeline with feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, RobustScaler,\n",
    "    PowerTransformer, QuantileTransformer,\n",
    "    OneHotEncoder, OrdinalEncoder, PolynomialFeatures,\n",
    "    FunctionTransformer, KBinsDiscretizer\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "    mutual_info_regression, f_regression, SelectKBest, RFECV\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "np.random.seed(42)\n",
    "print('Libraries loaded successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Numerical Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different scalers on skewed data\n",
    "data = np.random.exponential(scale=2, size=1000).reshape(-1, 1)\n",
    "\n",
    "scalers = {\n",
    "    'Original': None,\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler(),\n",
    "    'PowerTransformer': PowerTransformer(method='yeo-johnson'),\n",
    "    'QuantileTransformer': QuantileTransformer(output_distribution='normal'),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "for ax, (name, scaler) in zip(axes.ravel(), scalers.items()):\n",
    "    transformed = data if scaler is None else scaler.fit_transform(data)\n",
    "    ax.hist(transformed, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax.set_title(name)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction and polynomial features\n",
    "house_data = pd.DataFrame({\n",
    "    'length': [10, 15, 20, 12, 18],\n",
    "    'width': [8, 10, 12, 9, 11],\n",
    "    'floors': [1, 2, 1, 2, 3],\n",
    "})\n",
    "\n",
    "# Manual interactions\n",
    "house_data['area'] = house_data['length'] * house_data['width']\n",
    "house_data['volume'] = house_data['length'] * house_data['width'] * house_data['floors']\n",
    "\n",
    "# sklearn polynomial features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_poly = poly.fit_transform(house_data[['length', 'width']])\n",
    "print('Polynomial feature names:', poly.get_feature_names_out(['length', 'width']))\n",
    "print(pd.DataFrame(X_poly, columns=poly.get_feature_names_out(['length', 'width'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoding with smoothing\n",
    "n = 1000\n",
    "df = pd.DataFrame({\n",
    "    'city': np.random.choice(['NYC', 'LA', 'Chicago', 'Houston', 'Phoenix'], n),\n",
    "    'target': np.random.binomial(1, 0.3, n)\n",
    "})\n",
    "df.loc[df['city'] == 'NYC', 'target'] = np.random.binomial(1, 0.7, (df['city'] == 'NYC').sum())\n",
    "df.loc[df['city'] == 'LA', 'target'] = np.random.binomial(1, 0.2, (df['city'] == 'LA').sum())\n",
    "\n",
    "# Smoothed target encoding\n",
    "global_mean = df['target'].mean()\n",
    "smoothing = 10\n",
    "\n",
    "stats = df.groupby('city')['target'].agg(['mean', 'count'])\n",
    "stats['encoded'] = (stats['count'] * stats['mean'] + smoothing * global_mean) / (stats['count'] + smoothing)\n",
    "print('Target encoding with smoothing:')\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporal Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclical encoding for hours\n",
    "hours = pd.DataFrame({'hour': range(24)})\n",
    "hours['hour_sin'] = np.sin(2 * np.pi * hours['hour'] / 24)\n",
    "hours['hour_cos'] = np.cos(2 * np.pi * hours['hour'] / 24)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(hours['hour'], marker='o')\n",
    "axes[0].set_title('Linear: hour 23 and 0 are far apart')\n",
    "axes[0].set_xlabel('Index')\n",
    "axes[0].set_ylabel('Hour')\n",
    "\n",
    "scatter = axes[1].scatter(hours['hour_sin'], hours['hour_cos'], c=hours['hour'], cmap='twilight')\n",
    "for i in range(0, 24, 3):\n",
    "    axes[1].annotate(f'{i}h', (hours['hour_sin'][i], hours['hour_cos'][i]))\n",
    "axes[1].set_title('Cyclical: hour 23 and 0 are adjacent')\n",
    "axes[1].set_xlabel('sin(hour)')\n",
    "axes[1].set_ylabel('cos(hour)')\n",
    "axes[1].set_aspect('equal')\n",
    "plt.colorbar(scatter, ax=axes[1], label='Hour')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag and rolling features\n",
    "dates = pd.date_range('2023-01-01', '2023-12-31', freq='D')\n",
    "sales = 100 + 20 * np.sin(np.arange(len(dates)) * 2 * np.pi / 365) + np.random.normal(0, 10, len(dates))\n",
    "ts = pd.DataFrame({'date': dates, 'sales': sales})\n",
    "\n",
    "# Lag features\n",
    "for lag in [1, 7, 14]:\n",
    "    ts[f'lag_{lag}'] = ts['sales'].shift(lag)\n",
    "\n",
    "# Rolling features (shift first to prevent leakage)\n",
    "for window in [7, 14, 30]:\n",
    "    ts[f'rolling_mean_{window}'] = ts['sales'].shift(1).rolling(window).mean()\n",
    "    ts[f'rolling_std_{window}'] = ts['sales'].shift(1).rolling(window).std()\n",
    "\n",
    "print('Lag and rolling features (last 5 rows):')\n",
    "ts[['date', 'sales', 'lag_1', 'lag_7', 'rolling_mean_7', 'rolling_std_7']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "X, y = housing.data, housing.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Mutual Information scores\n",
    "mi_scores = mutual_info_regression(X_train, y_train, random_state=42)\n",
    "mi_df = pd.Series(mi_scores, index=X_train.columns).sort_values(ascending=True)\n",
    "\n",
    "# Random Forest feature importance\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_imp = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=True)\n",
    "\n",
    "# Lasso importance\n",
    "lasso = LassoCV(cv=5, random_state=42)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_imp = pd.Series(np.abs(lasso.coef_), index=X_train.columns).sort_values(ascending=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "mi_df.plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Mutual Information')\n",
    "rf_imp.plot(kind='barh', ax=axes[1], color='forestgreen')\n",
    "axes[1].set_title('Random Forest Importance')\n",
    "lasso_imp.plot(kind='barh', ax=axes[2], color='coral')\n",
    "axes[2].set_title('Lasso |Coefficient|')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. End-to-End Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom feature creation function\n",
    "def create_housing_features(X):\n",
    "    X = X.copy()\n",
    "    X['rooms_per_household'] = X['AveRooms'] / X['AveOccup'].clip(lower=0.1)\n",
    "    X['bedrooms_ratio'] = X['AveBedrms'] / X['AveRooms'].clip(lower=0.1)\n",
    "    X['population_density'] = X['Population'] / X['AveOccup'].clip(lower=0.1)\n",
    "    X['income_per_room'] = X['MedInc'] / X['AveRooms'].clip(lower=0.1)\n",
    "    return X\n",
    "\n",
    "feature_creator = FunctionTransformer(create_housing_features, validate=False)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('power', PowerTransformer(method='yeo-johnson'),\n",
    "     ['MedInc', 'Population', 'AveOccup']),\n",
    "    ('standard', StandardScaler(),\n",
    "     ['HouseAge', 'AveRooms', 'AveBedrms', 'Latitude', 'Longitude',\n",
    "      'rooms_per_household', 'bedrooms_ratio', 'population_density', 'income_per_room']),\n",
    "], remainder='drop')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', feature_creator),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', GradientBoostingRegressor(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)),\n",
    "])\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')\n",
    "print(f'Engineered CV R²: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})')\n",
    "\n",
    "# Baseline (no feature engineering)\n",
    "baseline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', GradientBoostingRegressor(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)),\n",
    "])\n",
    "baseline_scores = cross_val_score(baseline, X_train, y_train, cv=5, scoring='r2')\n",
    "print(f'Baseline CV R²:   {baseline_scores.mean():.4f} (+/- {baseline_scores.std():.4f})')\n",
    "print(f'Improvement:       {cv_scores.mean() - baseline_scores.mean():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
