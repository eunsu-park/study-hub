{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "This notebook demonstrates key feature engineering techniques:\n",
    "- Numerical transformations (scaling, binning, polynomial features)\n",
    "- Categorical encoding (target, frequency, hash encoding)\n",
    "- Date/time features (cyclical encoding, lag features)\n",
    "- Text features (TF-IDF, basic statistics)\n",
    "- Feature selection (filter, wrapper, embedded methods)\n",
    "- End-to-end pipeline with feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, RobustScaler,\n",
    "    PowerTransformer, QuantileTransformer,\n",
    "    OneHotEncoder, OrdinalEncoder, PolynomialFeatures,\n",
    "    FunctionTransformer, KBinsDiscretizer\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "    mutual_info_regression, f_regression, SelectKBest, RFECV\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "np.random.seed(42)\n",
    "print('Libraries loaded successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Numerical Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different scalers on skewed data\n",
    "data = np.random.exponential(scale=2, size=1000).reshape(-1, 1)\n",
    "\n",
    "scalers = {\n",
    "    'Original': None,\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler(),\n",
    "    'PowerTransformer': PowerTransformer(method='yeo-johnson'),\n",
    "    'QuantileTransformer': QuantileTransformer(output_distribution='normal'),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "for ax, (name, scaler) in zip(axes.ravel(), scalers.items()):\n",
    "    transformed = data if scaler is None else scaler.fit_transform(data)\n",
    "    ax.hist(transformed, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax.set_title(name)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction and polynomial features\n",
    "house_data = pd.DataFrame({\n",
    "    'length': [10, 15, 20, 12, 18],\n",
    "    'width': [8, 10, 12, 9, 11],\n",
    "    'floors': [1, 2, 1, 2, 3],\n",
    "})\n",
    "\n",
    "# Manual interactions\n",
    "house_data['area'] = house_data['length'] * house_data['width']\n",
    "house_data['volume'] = house_data['length'] * house_data['width'] * house_data['floors']\n",
    "\n",
    "# sklearn polynomial features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_poly = poly.fit_transform(house_data[['length', 'width']])\n",
    "print('Polynomial feature names:', poly.get_feature_names_out(['length', 'width']))\n",
    "print(pd.DataFrame(X_poly, columns=poly.get_feature_names_out(['length', 'width'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Target encoding with smoothing\nn = 1000\ndf = pd.DataFrame({\n    'city': np.random.choice(['NYC', 'LA', 'Chicago', 'Houston', 'Phoenix'], n),\n    'target': np.random.binomial(1, 0.3, n)\n})\ndf.loc[df['city'] == 'NYC', 'target'] = np.random.binomial(1, 0.7, (df['city'] == 'NYC').sum())\ndf.loc[df['city'] == 'LA', 'target'] = np.random.binomial(1, 0.2, (df['city'] == 'LA').sum())\n\n# Why: Smoothing blends the category mean with the global mean, preventing overfitting\n# on rare categories — without it, a city with 2 samples and 100% target rate would\n# get an artificially high encoding that doesn't generalize.\nglobal_mean = df['target'].mean()\nsmoothing = 10\n\nstats = df.groupby('city')['target'].agg(['mean', 'count'])\nstats['encoded'] = (stats['count'] * stats['mean'] + smoothing * global_mean) / (stats['count'] + smoothing)\nprint('Target encoding with smoothing:')\nprint(stats)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporal Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: Cyclical encoding with sin/cos maps periodic features onto a circle, so that\n# hour 23 and hour 0 are adjacent (distance ~0.26) rather than 23 units apart as\n# in raw linear encoding — this is critical for any periodic feature (hour, day, month).\nhours = pd.DataFrame({'hour': range(24)})\nhours['hour_sin'] = np.sin(2 * np.pi * hours['hour'] / 24)\nhours['hour_cos'] = np.cos(2 * np.pi * hours['hour'] / 24)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\naxes[0].plot(hours['hour'], marker='o')\naxes[0].set_title('Linear: hour 23 and 0 are far apart')\naxes[0].set_xlabel('Index')\naxes[0].set_ylabel('Hour')\n\nscatter = axes[1].scatter(hours['hour_sin'], hours['hour_cos'], c=hours['hour'], cmap='twilight')\nfor i in range(0, 24, 3):\n    axes[1].annotate(f'{i}h', (hours['hour_sin'][i], hours['hour_cos'][i]))\naxes[1].set_title('Cyclical: hour 23 and 0 are adjacent')\naxes[1].set_xlabel('sin(hour)')\naxes[1].set_ylabel('cos(hour)')\naxes[1].set_aspect('equal')\nplt.colorbar(scatter, ax=axes[1], label='Hour')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Lag and rolling features\ndates = pd.date_range('2023-01-01', '2023-12-31', freq='D')\nsales = 100 + 20 * np.sin(np.arange(len(dates)) * 2 * np.pi / 365) + np.random.normal(0, 10, len(dates))\nts = pd.DataFrame({'date': dates, 'sales': sales})\n\n# Lag features\nfor lag in [1, 7, 14]:\n    ts[f'lag_{lag}'] = ts['sales'].shift(lag)\n\n# Why: shift(1) before rolling() prevents target leakage — without the shift, today's\n# actual sales value would be included in its own rolling window, giving the model\n# illegitimate access to the target at prediction time.\nfor window in [7, 14, 30]:\n    ts[f'rolling_mean_{window}'] = ts['sales'].shift(1).rolling(window).mean()\n    ts[f'rolling_std_{window}'] = ts['sales'].shift(1).rolling(window).std()\n\nprint('Lag and rolling features (last 5 rows):')\nts[['date', 'sales', 'lag_1', 'lag_7', 'rolling_mean_7', 'rolling_std_7']].tail()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load California Housing dataset\nhousing = fetch_california_housing(as_frame=True)\nX, y = housing.data, housing.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Why: Comparing three different importance methods (MI, RF, Lasso) gives a more robust\n# picture — MI captures non-linear relationships, RF measures split usefulness, and\n# Lasso measures linear contribution. Features ranked high by all three are most reliable.\nmi_scores = mutual_info_regression(X_train, y_train, random_state=42)\nmi_df = pd.Series(mi_scores, index=X_train.columns).sort_values(ascending=True)\n\n# Random Forest feature importance\nrf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\nrf.fit(X_train, y_train)\nrf_imp = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=True)\n\n# Lasso importance\nlasso = LassoCV(cv=5, random_state=42)\nlasso.fit(X_train, y_train)\nlasso_imp = pd.Series(np.abs(lasso.coef_), index=X_train.columns).sort_values(ascending=True)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nmi_df.plot(kind='barh', ax=axes[0], color='steelblue')\naxes[0].set_title('Mutual Information')\nrf_imp.plot(kind='barh', ax=axes[1], color='forestgreen')\naxes[1].set_title('Random Forest Importance')\nlasso_imp.plot(kind='barh', ax=axes[2], color='coral')\naxes[2].set_title('Lasso |Coefficient|')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. End-to-End Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: Domain-driven ratio features (rooms_per_household, bedrooms_ratio, etc.) encode\n# meaningful real-world relationships that raw features alone cannot express — e.g.,\n# income_per_room captures affordability better than MedInc or AveRooms individually.\ndef create_housing_features(X):\n    X = X.copy()\n    X['rooms_per_household'] = X['AveRooms'] / X['AveOccup'].clip(lower=0.1)\n    X['bedrooms_ratio'] = X['AveBedrms'] / X['AveRooms'].clip(lower=0.1)\n    X['population_density'] = X['Population'] / X['AveOccup'].clip(lower=0.1)\n    X['income_per_room'] = X['MedInc'] / X['AveRooms'].clip(lower=0.1)\n    return X\n\nfeature_creator = FunctionTransformer(create_housing_features, validate=False)\n\n# Why: PowerTransformer (Yeo-Johnson) normalizes skewed features like Population and\n# AveOccup, which helps gradient-based models converge faster and improves predictions\n# for features with heavy-tailed distributions.\npreprocessor = ColumnTransformer([\n    ('power', PowerTransformer(method='yeo-johnson'),\n     ['MedInc', 'Population', 'AveOccup']),\n    ('standard', StandardScaler(),\n     ['HouseAge', 'AveRooms', 'AveBedrms', 'Latitude', 'Longitude',\n      'rooms_per_household', 'bedrooms_ratio', 'population_density', 'income_per_room']),\n], remainder='drop')\n\npipeline = Pipeline([\n    ('features', feature_creator),\n    ('preprocessor', preprocessor),\n    ('model', GradientBoostingRegressor(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)),\n])\n\n# Cross-validation\ncv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')\nprint(f'Engineered CV R²: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})')\n\n# Baseline (no feature engineering)\nbaseline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', GradientBoostingRegressor(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)),\n])\nbaseline_scores = cross_val_score(baseline, X_train, y_train, cv=5, scoring='r2')\nprint(f'Baseline CV R²:   {baseline_scores.mean():.4f} (+/- {baseline_scores.std():.4f})')\nprint(f'Improvement:       {cv_scores.mean() - baseline_scores.mean():.4f}')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}