{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Imbalanced Data\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Why accuracy fails for imbalanced datasets\n",
    "- PR curves vs ROC curves\n",
    "- SMOTE and sampling techniques\n",
    "- Cost-sensitive learning (class_weight)\n",
    "- Threshold optimization\n",
    "- Balanced ensemble methods\n",
    "\n",
    "**Requirements**: `pip install imbalanced-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import (\n    classification_report, f1_score, precision_score, recall_score,\n    precision_recall_curve, roc_curve, auc, average_precision_score,\n    matthews_corrcoef, ConfusionMatrixDisplay, make_scorer\n)\n\n# Why: weights=[0.95, 0.05] creates a 19:1 class imbalance, simulating real-world\n# scenarios like fraud detection or disease screening where positive cases are rare.\n# flip_y=0.01 adds a small amount of label noise to make the task more realistic.\nX, y = make_classification(\n    n_samples=5000, n_features=20, n_informative=10,\n    weights=[0.95, 0.05], random_state=42, flip_y=0.01\n)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint(f'Train: {Counter(y_train)}, Test: {Counter(y_test)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline and Why Accuracy Fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "y_proba = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print('Baseline (no imbalance handling):')\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
    "print(f'MCC: {matthews_corrcoef(y_test, y_pred):.4f}')\n",
    "print(f'PR-AUC: {average_precision_score(y_test, y_proba):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PR Curve vs ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: ROC curves can be misleadingly optimistic for imbalanced data because the large\n# number of true negatives keeps FPR low. PR curves focus only on the positive class\n# (precision and recall), giving a more honest view of minority-class performance.\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nfpr, tpr, _ = roc_curve(y_test, y_proba)\naxes[0].plot(fpr, tpr, 'b-', lw=2, label=f'ROC AUC={auc(fpr,tpr):.3f}')\naxes[0].plot([0,1],[0,1],'k--',alpha=0.5)\naxes[0].set_xlabel('FPR'); axes[0].set_ylabel('TPR')\naxes[0].set_title('ROC Curve'); axes[0].legend()\n\nprec, rec, _ = precision_recall_curve(y_test, y_proba)\naxes[1].plot(rec, prec, 'r-', lw=2, label=f'PR AUC={auc(rec,prec):.3f}')\naxes[1].axhline(y_test.mean(), color='k', ls='--', alpha=0.5)\naxes[1].set_xlabel('Recall'); axes[1].set_ylabel('Precision')\naxes[1].set_title('PR Curve'); axes[1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sampling Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.pipeline import Pipeline as ImbPipeline\n\n# Why: SMOTE generates synthetic minority samples by interpolating between nearest\n# neighbors, which expands the decision boundary (unlike RandomOverSampler that just\n# duplicates). Borderline-SMOTE focuses synthesis near the decision boundary where it\n# matters most. SMOTE-ENN combines oversampling with Edited Nearest Neighbors cleaning\n# to remove noisy samples created near the majority class.\nstrategies = {\n    'No Sampling': None,\n    'Random Over': RandomOverSampler(random_state=42),\n    'Random Under': RandomUnderSampler(random_state=42),\n    'SMOTE': SMOTE(random_state=42),\n    'Borderline-SMOTE': BorderlineSMOTE(random_state=42),\n    'SMOTE-ENN': SMOTEENN(random_state=42),\n}\n\nresults = {}\nfor name, sampler in strategies.items():\n    if sampler is None:\n        pipe = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n    else:\n        pipe = ImbPipeline([\n            ('sampler', sampler),\n            ('clf', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))\n        ])\n    cv = StratifiedKFold(5, shuffle=True, random_state=42)\n    scores = cross_validate(pipe, X_train, y_train, cv=cv,\n                           scoring={'f1':'f1','pr_auc':'average_precision'}, n_jobs=-1)\n    results[name] = {\n        'F1': scores['test_f1'].mean(),\n        'PR-AUC': scores['test_pr_auc'].mean()\n    }\n\npd.DataFrame(results).T.sort_values('F1', ascending=False).round(4)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cost-Sensitive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: class_weight='balanced' adjusts the loss function to penalize minority-class\n# misclassification more heavily (weight = n_samples / (n_classes * n_class_samples)).\n# This is simpler than resampling and works within a single estimator with no extra library.\nrf_balanced = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1)\nrf_balanced.fit(X_train, y_train)\ny_pred_bal = rf_balanced.predict(X_test)\n\nprint('class_weight=\"balanced\":')\nprint(classification_report(y_test, y_pred_bal, target_names=['Neg', 'Pos']))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: The default 0.5 threshold is suboptimal for imbalanced data â€” sweeping thresholds\n# finds the operating point that maximizes F1, trading off precision vs recall based on\n# the actual class distribution rather than using an arbitrary cutoff.\ny_proba = rf.predict_proba(X_test)[:, 1]\nthresholds = np.arange(0.05, 0.95, 0.05)\nmetrics = []\nfor t in thresholds:\n    yp = (y_proba >= t).astype(int)\n    metrics.append({'threshold': t, 'f1': f1_score(y_test, yp, zero_division=0),\n                    'precision': precision_score(y_test, yp, zero_division=0),\n                    'recall': recall_score(y_test, yp)})\n\nmdf = pd.DataFrame(metrics)\nfig, ax = plt.subplots(figsize=(10, 5))\nfor col in ['precision', 'recall', 'f1']:\n    ax.plot(mdf['threshold'], mdf[col], lw=2, label=col)\nbest = mdf.loc[mdf['f1'].idxmax()]\nax.axvline(best['threshold'], color='red', ls='--', label=f'Best F1={best[\"f1\"]:.3f} @ {best[\"threshold\"]:.2f}')\nax.set_xlabel('Threshold'); ax.set_ylabel('Score')\nax.set_title('Threshold Optimization'); ax.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Balanced Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
    "\n",
    "models = {\n",
    "    'RF (default)': RandomForestClassifier(100, random_state=42, n_jobs=-1),\n",
    "    'RF (balanced)': RandomForestClassifier(100, class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    'BalancedRF': BalancedRandomForestClassifier(100, random_state=42, n_jobs=-1),\n",
    "    'EasyEnsemble': EasyEnsembleClassifier(10, random_state=42, n_jobs=-1),\n",
    "}\n",
    "\n",
    "res = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    yp = model.predict(X_test)\n",
    "    ypr = model.predict_proba(X_test)[:, 1]\n",
    "    res[name] = {'F1': f1_score(y_test,yp), 'PR-AUC': average_precision_score(y_test,ypr),\n",
    "                 'MCC': matthews_corrcoef(y_test,yp)}\n",
    "\n",
    "pd.DataFrame(res).T.round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}