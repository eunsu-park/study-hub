{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Kernel Tricks Deep Dive\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the mathematics behind the kernel trick\n",
    "- Visualize kernel-induced feature spaces\n",
    "- Compare kernel performance on various datasets\n",
    "- Implement a custom kernel function\n",
    "- Analyze the kernel (Gram) matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics.pairwise import rbf_kernel, polynomial_kernel, linear_kernel\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Kernels? — The Feature Map Idea\n",
    "\n",
    "A kernel function computes the inner product in a high-dimensional feature space **without explicitly mapping the data**:\n",
    "\n",
    "$$K(x, z) = \\langle \\phi(x), \\phi(z) \\rangle$$\n",
    "\n",
    "This is the **kernel trick** — it avoids the computational cost of working in the (possibly infinite-dimensional) feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate: non-linearly separable data becomes separable in higher dimensions\nX_circles, y_circles = make_circles(n_samples=300, noise=0.05, factor=0.4, random_state=42)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Original 2D space — not linearly separable\naxes[0].scatter(X_circles[y_circles==0, 0], X_circles[y_circles==0, 1], c='blue', label='Class 0', alpha=0.6)\naxes[0].scatter(X_circles[y_circles==1, 0], X_circles[y_circles==1, 1], c='red', label='Class 1', alpha=0.6)\naxes[0].set_title('Original 2D Space\\n(Not linearly separable)')\naxes[0].legend()\naxes[0].set_aspect('equal')\n\n# Why: This explicit mapping phi(x1,x2) = (x1^2, sqrt(2)*x1*x2, x2^2) shows\n# what the polynomial kernel computes implicitly. In 3D, the circles become\n# linearly separable — the kernel trick does this without materializing the map.\nX_mapped = np.column_stack([\n    X_circles[:, 0]**2,\n    np.sqrt(2) * X_circles[:, 0] * X_circles[:, 1],\n    X_circles[:, 1]**2\n])\n\n# 3D feature space\nax3d = fig.add_subplot(132, projection='3d')\nax3d.scatter(X_mapped[y_circles==0, 0], X_mapped[y_circles==0, 1], X_mapped[y_circles==0, 2], c='blue', alpha=0.4)\nax3d.scatter(X_mapped[y_circles==1, 0], X_mapped[y_circles==1, 1], X_mapped[y_circles==1, 2], c='red', alpha=0.4)\nax3d.set_title('Mapped to 3D\\n(Linearly separable!)')\nax3d.set_xlabel('x1²')\nax3d.set_ylabel('√2·x1·x2')\nax3d.set_zlabel('x2²')\naxes[1].set_visible(False)\n\n# SVM with RBF kernel (does this implicitly)\nclf = SVC(kernel='rbf', gamma='scale')\nclf.fit(X_circles, y_circles)\nxx, yy = np.meshgrid(np.linspace(-1.5, 1.5, 200), np.linspace(-1.5, 1.5, 200))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\naxes[2].contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\naxes[2].scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap='coolwarm', edgecolors='k', s=30)\naxes[2].set_title(f'RBF Kernel SVM\\n(Acc: {clf.score(X_circles, y_circles):.3f})')\naxes[2].set_aspect('equal')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kernel Functions Compared\n",
    "\n",
    "| Kernel | Formula | Feature Space | Use Case |\n",
    "|--------|---------|---------------|----------|\n",
    "| Linear | $K(x,z) = x^T z$ | Same as input | Linearly separable |\n",
    "| Polynomial | $K(x,z) = (\\gamma x^T z + r)^d$ | Finite dim | Polynomial boundaries |\n",
    "| RBF (Gaussian) | $K(x,z) = \\exp(-\\gamma \\|x-z\\|^2)$ | Infinite dim | General non-linear |\n",
    "| Sigmoid | $K(x,z) = \\tanh(\\gamma x^T z + r)$ | — | Neural network analog |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all kernels on three different datasets\n",
    "datasets = [\n",
    "    ('Linearly separable', make_classification(n_samples=200, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, random_state=42)),\n",
    "    ('Moons', make_moons(n_samples=200, noise=0.15, random_state=42)),\n",
    "    ('Circles', make_circles(n_samples=200, noise=0.1, factor=0.4, random_state=42)),\n",
    "]\n",
    "\n",
    "kernels = [\n",
    "    ('Linear', {'kernel': 'linear'}),\n",
    "    ('Poly (d=3)', {'kernel': 'poly', 'degree': 3, 'gamma': 'scale'}),\n",
    "    ('RBF', {'kernel': 'rbf', 'gamma': 'scale'}),\n",
    "    ('Sigmoid', {'kernel': 'sigmoid', 'gamma': 'scale'}),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "\n",
    "for row, (ds_name, (X, y)) in enumerate(datasets):\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    for col, (k_name, k_params) in enumerate(kernels):\n",
    "        ax = axes[row, col]\n",
    "        clf = SVC(**k_params, C=1.0)\n",
    "        clf.fit(X, y)\n",
    "        score = clf.score(X, y)\n",
    "        \n",
    "        xx, yy = np.meshgrid(np.linspace(X[:,0].min()-0.5, X[:,0].max()+0.5, 150),\n",
    "                             np.linspace(X[:,1].min()-0.5, X[:,1].max()+0.5, 150))\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "        ax.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm', edgecolors='k', s=20)\n",
    "        ax.set_title(f'{k_name}\\nAcc: {score:.3f}, SVs: {len(clf.support_vectors_)}')\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(ds_name, fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kernel (Gram) Matrix Visualization\n",
    "\n",
    "The Gram matrix $K_{ij} = K(x_i, x_j)$ captures pairwise similarities between all data points. A valid kernel must produce a positive semi-definite Gram matrix (Mercer's condition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate data and compute kernel matrices\nX, y = make_moons(n_samples=100, noise=0.1, random_state=42)\nX = StandardScaler().fit_transform(X)\n\n# Sort by class for block structure visualization\norder = np.argsort(y)\nX_sorted = X[order]\ny_sorted = y[order]\n\n# Compute kernel matrices\nK_linear = linear_kernel(X_sorted)\nK_poly = polynomial_kernel(X_sorted, degree=3, gamma=1, coef0=1)\nK_rbf_low = rbf_kernel(X_sorted, gamma=0.1)\nK_rbf_high = rbf_kernel(X_sorted, gamma=10)\n\nfig, axes = plt.subplots(1, 4, figsize=(20, 5))\nmatrices = [\n    ('Linear', K_linear),\n    ('Poly (d=3)', K_poly),\n    ('RBF (gamma=0.1)', K_rbf_low),\n    ('RBF (gamma=10)', K_rbf_high),\n]\n\nfor ax, (name, K) in zip(axes, matrices):\n    im = ax.imshow(K, cmap='viridis', aspect='auto')\n    ax.set_title(f'{name}\\nrank={np.linalg.matrix_rank(K)}')\n    ax.axhline(y=np.sum(y_sorted==0)-0.5, color='red', linewidth=1, linestyle='--')\n    ax.axvline(x=np.sum(y_sorted==0)-0.5, color='red', linewidth=1, linestyle='--')\n    plt.colorbar(im, ax=ax, shrink=0.8)\n\nplt.suptitle('Kernel (Gram) Matrices — Red lines separate classes', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Why: A valid kernel (Mercer's condition) must produce a PSD Gram matrix.\n# If min eigenvalue < 0, the \"kernel\" is invalid and SVM optimization may fail.\nfor name, K in matrices:\n    eigenvalues = np.linalg.eigvalsh(K)\n    is_psd = np.all(eigenvalues >= -1e-10)\n    print(f'{name}: PSD={is_psd}, min eigenvalue={eigenvalues.min():.6f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RBF Gamma — Effect on Decision Boundary\n",
    "\n",
    "The gamma parameter controls the \"reach\" of each support vector:\n",
    "\n",
    "$$K(x, z) = \\exp(-\\gamma \\|x - z\\|^2)$$\n",
    "\n",
    "- **Small gamma**: each point has wide influence → smooth boundary\n",
    "- **Large gamma**: each point has narrow influence → complex boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "gammas = [0.01, 0.1, 1, 10, 100]\n",
    "fig, axes = plt.subplots(1, 5, figsize=(25, 5))\n",
    "\n",
    "for ax, gamma in zip(axes, gammas):\n",
    "    clf = SVC(kernel='rbf', gamma=gamma, C=1)\n",
    "    clf.fit(X, y)\n",
    "    cv_score = cross_val_score(clf, X, y, cv=5).mean()\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(-3, 3, 200), np.linspace(-3, 3, 200))\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, levels=20, cmap='coolwarm', alpha=0.5)\n",
    "    ax.contour(xx, yy, Z, levels=[0], colors='black', linewidths=2)\n",
    "    ax.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm', edgecolors='k', s=30)\n",
    "    ax.scatter(clf.support_vectors_[:,0], clf.support_vectors_[:,1],\n",
    "               s=80, facecolors='none', edgecolors='lime', linewidths=1.5)\n",
    "    ax.set_title(f'gamma={gamma}\\nSVs={len(clf.support_vectors_)}\\nCV={cv_score:.3f}')\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "\n",
    "plt.suptitle('RBF Kernel: Effect of gamma on Decision Boundary', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Polynomial Kernel — Degree Effect\n",
    "\n",
    "$$K(x, z) = (\\gamma x^T z + r)^d$$\n",
    "\n",
    "The degree $d$ controls the complexity of the polynomial boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.15, random_state=42)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "degrees = [1, 2, 3, 5, 10]\n",
    "fig, axes = plt.subplots(1, 5, figsize=(25, 5))\n",
    "\n",
    "for ax, degree in zip(axes, degrees):\n",
    "    clf = SVC(kernel='poly', degree=degree, gamma='scale', coef0=1, C=1)\n",
    "    clf.fit(X, y)\n",
    "    cv_score = cross_val_score(clf, X, y, cv=5).mean()\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(-3, 3, 200), np.linspace(-3, 3, 200))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    ax.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm', edgecolors='k', s=30)\n",
    "    ax.set_title(f'Poly d={degree}\\nSVs={len(clf.support_vectors_)}\\nCV={cv_score:.3f}')\n",
    "\n",
    "plt.suptitle('Polynomial Kernel: Effect of Degree', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Kernel Function\n",
    "\n",
    "sklearn allows custom kernels via `kernel='precomputed'` or a callable. A valid kernel must be symmetric and positive semi-definite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: The Laplacian kernel uses L1 (Manhattan) distance instead of L2 (Euclidean).\n# L1 is more robust to outliers in individual features. kernel='precomputed'\n# tells SVM to use a pre-built Gram matrix instead of computing kernels internally.\ndef laplacian_kernel(X, Y, gamma=1.0):\n    \"\"\"Laplacian kernel using L1 (Manhattan) distance.\"\"\"\n    from scipy.spatial.distance import cdist\n    dists = cdist(X, Y, metric='cityblock')  # L1 distance\n    return np.exp(-gamma * dists)\n\n# Compare standard RBF vs Laplacian on moons data\nX, y = make_moons(n_samples=200, noise=0.15, random_state=42)\nX = StandardScaler().fit_transform(X)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# RBF kernel\nclf_rbf = SVC(kernel='rbf', gamma=1.0, C=1)\nclf_rbf.fit(X, y)\n\n# Laplacian kernel (precomputed)\nK_train = laplacian_kernel(X, X, gamma=1.0)\nclf_lap = SVC(kernel='precomputed', C=1)\nclf_lap.fit(K_train, y)\n\n# Plot decision boundaries\nxx, yy = np.meshgrid(np.linspace(-3, 3, 150), np.linspace(-3, 3, 150))\ngrid = np.c_[xx.ravel(), yy.ravel()]\n\n# RBF\nZ_rbf = clf_rbf.predict(grid).reshape(xx.shape)\naxes[0].contourf(xx, yy, Z_rbf, alpha=0.3, cmap='coolwarm')\naxes[0].scatter(X[:,0], X[:,1], c=y, cmap='coolwarm', edgecolors='k', s=30)\naxes[0].set_title(f'RBF Kernel (L2)\\nAcc: {clf_rbf.score(X, y):.3f}')\n\n# Laplacian\nK_grid = laplacian_kernel(grid, X, gamma=1.0)\nZ_lap = clf_lap.predict(K_grid).reshape(xx.shape)\naxes[1].contourf(xx, yy, Z_lap, alpha=0.3, cmap='coolwarm')\naxes[1].scatter(X[:,0], X[:,1], c=y, cmap='coolwarm', edgecolors='k', s=30)\naxes[1].set_title(f'Laplacian Kernel (L1)\\nAcc: {clf_lap.predict(K_train).mean():.3f}')\n\n# Kernel matrices comparison\nK_rbf_mat = rbf_kernel(X[:50], gamma=1.0)\nK_lap_mat = laplacian_kernel(X[:50], X[:50], gamma=1.0)\naxes[2].imshow(K_rbf_mat - K_lap_mat, cmap='RdBu', aspect='auto')\naxes[2].set_title('RBF - Laplacian\\n(Kernel matrix difference)')\nplt.colorbar(axes[2].images[0], ax=axes[2])\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Kernel Selection Guide\n",
    "\n",
    "Practical guidelines for choosing the right kernel:\n",
    "\n",
    "| Scenario | Recommended Kernel | Why |\n",
    "|----------|-------------------|-----|\n",
    "| n_features >> n_samples | Linear | High-D data is often linearly separable |\n",
    "| n_samples >> n_features | RBF | Flexible, works well in low-D |\n",
    "| Text classification (TF-IDF) | Linear | Sparse, high-dimensional |\n",
    "| Image classification | RBF or Chi-squared | Non-linear patterns |\n",
    "| Known polynomial relationship | Polynomial | Matches data structure |\n",
    "| Default choice | RBF | Most versatile |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Kernel trick** avoids explicit high-dimensional mapping\n",
    "2. **Gram matrix** encodes all pairwise similarities\n",
    "3. **RBF** is the default choice — works well on most datasets\n",
    "4. **gamma** controls overfitting: high gamma = complex boundary\n",
    "5. **Custom kernels** must be symmetric and positive semi-definite\n",
    "6. Always **scale features** before applying SVM with any kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}