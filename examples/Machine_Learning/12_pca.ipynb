{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. 차원 축소 (PCA, t-SNE)\n",
    "\n",
    "## 학습 목표\n",
    "- PCA(주성분 분석) 이해\n",
    "- 차원 축소의 목적과 활용\n",
    "- t-SNE를 이용한 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PCA 기본 개념"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris 데이터\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "print(f\"Features: {iris.feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: PCA finds directions of maximum variance, so features must be standardized first;\n# otherwise high-magnitude features dominate the principal components regardless of\n# their actual informational content.\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# PCA 적용\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nprint(f\"Reduced shape: {X_pca.shape}\")\nprint(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\nprint(f\"Total explained variance: {sum(pca.explained_variance_ratio_):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 결과 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "for i, (color, name) in enumerate(zip(colors, iris.target_names)):\n",
    "    idx = y == i\n",
    "    plt.scatter(X_pca[idx, 0], X_pca[idx, 1], c=color, label=name, \n",
    "                alpha=0.7, edgecolors='black')\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "plt.title('PCA - Iris Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 설명 분산 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 전체 성분 PCA\npca_full = PCA()\npca_full.fit(X_scaled)\n\n# Why: The cumulative explained variance curve shows how many components are needed\n# to retain a target amount of information — the 95% threshold is a common heuristic\n# that balances dimensionality reduction with minimal information loss.\ncumsum = np.cumsum(pca_full.explained_variance_ratio_)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 개별 설명 분산\naxes[0].bar(range(1, len(pca_full.explained_variance_ratio_)+1), \n            pca_full.explained_variance_ratio_)\naxes[0].set_xlabel('Principal Component')\naxes[0].set_ylabel('Explained Variance Ratio')\naxes[0].set_title('Explained Variance by Component')\naxes[0].grid(True, alpha=0.3)\n\n# 누적 설명 분산\naxes[1].plot(range(1, len(cumsum)+1), cumsum, 'b-o')\naxes[1].axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\naxes[1].set_xlabel('Number of Components')\naxes[1].set_ylabel('Cumulative Explained Variance')\naxes[1].set_title('Cumulative Explained Variance')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 주성분 해석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주성분 로딩(가중치)\n",
    "loadings = pd.DataFrame(\n",
    "    pca_full.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(len(iris.feature_names))],\n",
    "    index=iris.feature_names\n",
    ")\n",
    "\n",
    "print(\"=== Principal Component Loadings ===\")\n",
    "print(loadings.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로딩 히트맵\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(loadings.iloc[:, :2], annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('PCA Component Loadings')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. t-SNE 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digits 데이터셋 (고차원)\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(f\"Digits shape: {X_digits.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_digits))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# PCA vs t-SNE 비교\n# PCA\npca_digits = PCA(n_components=2)\nX_pca_digits = pca_digits.fit_transform(X_digits)\n\n# Why: t-SNE preserves local neighborhood structure by converting pairwise distances\n# to conditional probabilities, making it far superior to PCA for visualizing clusters\n# in high-dimensional data. perplexity=30 controls the effective neighborhood size.\ntsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\nX_tsne = tsne.fit_transform(X_digits)\n\nprint(\"Transformation complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비교 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# PCA\n",
    "scatter1 = axes[0].scatter(X_pca_digits[:, 0], X_pca_digits[:, 1], \n",
    "                           c=y_digits, cmap='tab10', alpha=0.6, s=20)\n",
    "axes[0].set_title('PCA - Digits Dataset')\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Digit')\n",
    "\n",
    "# t-SNE\n",
    "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                           c=y_digits, cmap='tab10', alpha=0.6, s=20)\n",
    "axes[1].set_title('t-SNE - Digits Dataset')\n",
    "axes[1].set_xlabel('t-SNE 1')\n",
    "axes[1].set_ylabel('t-SNE 2')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Digit')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PCA를 이용한 노이즈 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 노이즈 있는 Digits 이미지\nnp.random.seed(42)\nnoise = np.random.normal(0, 4, X_digits.shape)\nX_noisy = X_digits + noise\n\n# Why: Using only 20 principal components acts as a low-pass filter — noise lives in\n# the low-variance components, so inverse_transform reconstructs the signal without\n# the noise captured by the discarded components.\npca_denoise = PCA(n_components=20)\nX_reconstructed = pca_denoise.inverse_transform(\n    pca_denoise.fit_transform(X_noisy)\n)\n\n# 결과 시각화\nfig, axes = plt.subplots(3, 10, figsize=(15, 5))\n\nfor i in range(10):\n    # 원본\n    axes[0, i].imshow(X_digits[i].reshape(8, 8), cmap='gray')\n    axes[0, i].axis('off')\n    if i == 0:\n        axes[0, i].set_title('Original')\n    \n    # 노이즈\n    axes[1, i].imshow(X_noisy[i].reshape(8, 8), cmap='gray')\n    axes[1, i].axis('off')\n    if i == 0:\n        axes[1, i].set_title('Noisy')\n    \n    # 복원\n    axes[2, i].imshow(X_reconstructed[i].reshape(8, 8), cmap='gray')\n    axes[2, i].axis('off')\n    if i == 0:\n        axes[2, i].set_title('Denoised')\n\nplt.suptitle('PCA Denoising (20 components)', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "### PCA vs t-SNE\n",
    "\n",
    "| 특성 | PCA | t-SNE |\n",
    "|------|-----|-------|\n",
    "| 목적 | 차원 축소, 분산 최대화 | 시각화, 군집 구조 |\n",
    "| 선형성 | 선형 변환 | 비선형 변환 |\n",
    "| 속도 | 빠름 | 느림 |\n",
    "| 재현성 | 결정적 | random_state 필요 |\n",
    "| 해석 | 가능 (로딩) | 어려움 |\n",
    "\n",
    "### 활용\n",
    "- **PCA**: 전처리, 노이즈 제거, 다중공선성 해소\n",
    "- **t-SNE**: 고차원 데이터 시각화, 군집 탐색"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}