{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. 클러스터링 (Clustering)\n",
    "\n",
    "## 학습 목표\n",
    "- K-Means 클러스터링 이해\n",
    "- DBSCAN 알고리즘\n",
    "- 클러스터 평가 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K-Means 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성\n",
    "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.6, edgecolors='black')\n",
    "plt.title('Original Data (Unlabeled)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# K-Means 학습\n# Why: n_init=10 runs the algorithm 10 times with different centroid seeds and picks\n# the best result, mitigating K-Means' sensitivity to initial centroid placement.\nkmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\ny_kmeans = kmeans.fit_predict(X)\n\n# 결과 시각화\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='viridis', \n                      alpha=0.6, edgecolors='black')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            c='red', marker='X', s=200, label='Centroids')\nplt.title('K-Means Clustering (K=4)')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.colorbar(scatter, label='Cluster')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"Inertia: {kmeans.inertia_:.2f}\")\nprint(f\"Silhouette Score: {silhouette_score(X, y_kmeans):.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 최적의 K 찾기 (Elbow Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Elbow Method\n# Why: We sweep K=2..10 and look for the \"elbow\" in inertia where adding more clusters\n# gives diminishing returns — the elbow indicates the natural cluster count.\ninertias = []\nsilhouettes = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n    km.fit(X)\n    inertias.append(km.inertia_)\n    silhouettes.append(silhouette_score(X, km.labels_))\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Elbow Plot\naxes[0].plot(K_range, inertias, 'b-o')\naxes[0].set_xlabel('Number of Clusters (K)')\naxes[0].set_ylabel('Inertia')\naxes[0].set_title('Elbow Method')\naxes[0].grid(True, alpha=0.3)\n\n# Silhouette Score\naxes[1].plot(K_range, silhouettes, 'g-o')\naxes[1].set_xlabel('Number of Clusters (K)')\naxes[1].set_ylabel('Silhouette Score')\naxes[1].set_title('Silhouette Score vs K')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 비구형 데이터 생성\nX_moons, y_moons = make_moons(n_samples=300, noise=0.05, random_state=42)\n\n# K-Means vs DBSCAN 비교\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# 원본 데이터\naxes[0].scatter(X_moons[:, 0], X_moons[:, 1], alpha=0.6, edgecolors='black')\naxes[0].set_title('Original Data (Moons)')\naxes[0].grid(True, alpha=0.3)\n\n# K-Means\nkm_moons = KMeans(n_clusters=2, random_state=42, n_init=10)\ny_km_moons = km_moons.fit_predict(X_moons)\naxes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=y_km_moons, cmap='viridis', \n                alpha=0.6, edgecolors='black')\naxes[1].set_title('K-Means (K=2)')\naxes[1].grid(True, alpha=0.3)\n\n# Why: DBSCAN groups points by density rather than distance to centroids, so it can\n# discover arbitrarily-shaped clusters (like these crescents) that K-Means cannot.\n# eps=0.2 defines the neighborhood radius; min_samples=5 sets the core-point threshold.\ndbscan = DBSCAN(eps=0.2, min_samples=5)\ny_dbscan = dbscan.fit_predict(X_moons)\naxes[2].scatter(X_moons[:, 0], X_moons[:, 1], c=y_dbscan, cmap='viridis', \n                alpha=0.6, edgecolors='black')\naxes[2].set_title('DBSCAN (eps=0.2, min_samples=5)')\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"K-Means Silhouette: {silhouette_score(X_moons, y_km_moons):.4f}\")\nprint(f\"DBSCAN Silhouette: {silhouette_score(X_moons, y_dbscan):.4f}\")\nprint(f\"DBSCAN found {len(set(y_dbscan)) - (1 if -1 in y_dbscan else 0)} clusters\")\nprint(f\"DBSCAN noise points: {sum(y_dbscan == -1)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 계층적 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from scipy.cluster.hierarchy import dendrogram, linkage\n\n# 덴드로그램\nX_small = X[:50]  # 시각화를 위해 일부 데이터만 사용\n# Why: Ward linkage minimizes within-cluster variance at each merge, producing compact,\n# spherical clusters — it is the most commonly used linkage for Euclidean data.\nlinkage_matrix = linkage(X_small, method='ward')\n\nplt.figure(figsize=(15, 7))\ndendrogram(linkage_matrix)\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Sample Index')\nplt.ylabel('Distance')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agglomerative Clustering\n",
    "agg = AgglomerativeClustering(n_clusters=4)\n",
    "y_agg = agg.fit_predict(X)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_agg, cmap='viridis', alpha=0.6, edgecolors='black')\n",
    "plt.title('Agglomerative Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score(X, y_agg):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 실제 데이터 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.datasets import load_iris\n\n# Iris 데이터 클러스터링\niris = load_iris()\nX_iris = iris.data\n\n# Why: Scaling is essential before K-Means because it uses Euclidean distance —\n# without scaling, features with larger ranges dominate the distance calculation.\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_iris)\n\n# K-Means\nkmeans_iris = KMeans(n_clusters=3, random_state=42, n_init=10)\ny_kmeans_iris = kmeans_iris.fit_predict(X_scaled)\n\n# 결과 비교\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 실제 레이블\naxes[0].scatter(X_iris[:, 0], X_iris[:, 1], c=iris.target, cmap='viridis', \n                alpha=0.6, edgecolors='black')\naxes[0].set_title('True Labels')\naxes[0].set_xlabel('Sepal Length')\naxes[0].set_ylabel('Sepal Width')\n\n# K-Means 결과\naxes[1].scatter(X_iris[:, 0], X_iris[:, 1], c=y_kmeans_iris, cmap='viridis', \n                alpha=0.6, edgecolors='black')\naxes[1].set_title('K-Means Clustering')\naxes[1].set_xlabel('Sepal Length')\naxes[1].set_ylabel('Sepal Width')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Silhouette Score: {silhouette_score(X_scaled, y_kmeans_iris):.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "### 알고리즘 비교\n",
    "\n",
    "| 알고리즘 | 장점 | 단점 |\n",
    "|---------|------|------|\n",
    "| K-Means | 빠름, 간단 | K 지정 필요, 구형 클러스터 가정 |\n",
    "| DBSCAN | K 불필요, 비구형 가능, 노이즈 처리 | eps, min_samples 설정 |\n",
    "| Hierarchical | 덴드로그램, 다양한 K | 대규모 데이터에 느림 |\n",
    "\n",
    "### 평가 지표\n",
    "- **Silhouette Score**: -1~1, 높을수록 좋음\n",
    "- **Inertia**: 클러스터 내 분산, 낮을수록 좋음\n",
    "- **Calinski-Harabasz**: 클러스터 간/내 분산 비율"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}