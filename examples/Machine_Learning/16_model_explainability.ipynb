{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Explainability\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Permutation importance\n",
    "- SHAP (TreeExplainer, summary/waterfall/dependence plots)\n",
    "- LIME (Local explanations)\n",
    "- Partial Dependence Plots (PDP) and ICE\n",
    "- SHAP vs LIME comparison\n",
    "\n",
    "**Requirements**: `pip install shap lime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "\n",
    "# Load data and train model\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "X, y = housing.data, housing.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "gb = GradientBoostingRegressor(n_estimators=200, max_depth=5, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "print(f'Test R²: {gb.score(X_test, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: Permutation importance measures the actual impact on test performance when a\n# feature's values are shuffled — unlike built-in feature_importances_ (which reflect\n# training split frequency), permutation importance is model-agnostic and computed on\n# held-out data, making it a more honest measure of predictive contribution.\nperm = permutation_importance(gb, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\nperm_imp = pd.Series(perm.importances_mean, index=X_train.columns).sort_values(ascending=True)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nperm_imp.plot(kind='barh', xerr=perm.importances_std, ax=ax, color='coral')\nax.set_title('Permutation Importance')\nax.set_xlabel('Decrease in R²')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import shap\n\n# Why: TreeExplainer uses the tree structure to compute exact SHAP values in polynomial\n# time (vs. exponential for KernelSHAP), making it the preferred explainer for tree-based\n# models. SHAP values satisfy additivity: base_value + sum(shap_values) = prediction.\nexplainer = shap.TreeExplainer(gb)\nshap_values = explainer.shap_values(X_test)\n\n# Verify additivity\npred = gb.predict(X_test.iloc[[0]])[0]\nshap_sum = explainer.expected_value + shap_values[0].sum()\nprint(f'Prediction: {pred:.4f}, SHAP sum: {shap_sum:.4f}, diff: {abs(pred-shap_sum):.6f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot (global)\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall plot (local - single prediction)\n",
    "shap.plots.waterfall(shap.Explanation(\n",
    "    values=shap_values[0],\n",
    "    base_values=explainer.expected_value,\n",
    "    data=X_test.iloc[0],\n",
    "    feature_names=X_test.columns.tolist()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependence plot\n",
    "shap.dependence_plot('MedInc', shap_values, X_test, interaction_index='AveOccup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import lime\nimport lime.lime_tabular\n\n# Why: LIME builds a local linear approximation around each individual prediction,\n# making it interpretable for any black-box model. Unlike SHAP (global consistency),\n# LIME focuses on local fidelity — the explanation is faithful in the neighborhood\n# of this specific instance, not necessarily globally.\nlime_explainer = lime.lime_tabular.LimeTabularExplainer(\n    training_data=X_train.values,\n    feature_names=X_train.columns.tolist(),\n    mode='regression'\n)\n\nexplanation = lime_explainer.explain_instance(\n    X_test.iloc[0].values, gb.predict, num_features=8\n)\n\nprint(f'Prediction: {gb.predict(X_test.iloc[[0]])[0]:.4f}')\nprint('\\nLIME contributions:')\nfor feat, weight in explanation.as_list():\n    print(f'  {feat}: {weight:+.4f}')\n\nexplanation.as_pyplot_figure()\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Partial Dependence Plots (PDP + ICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: kind='both' overlays Individual Conditional Expectation (ICE) curves on the PDP.\n# ICE lines reveal heterogeneous effects hidden by the average PDP — if ICE lines\n# diverge, the feature's effect varies across subpopulations (interaction effects).\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nPartialDependenceDisplay.from_estimator(\n    gb, X_test.iloc[:200], ['MedInc', 'AveOccup', 'Latitude'],\n    kind='both',\n    ax=axes,\n    ice_lines_kw={'alpha': 0.1, 'color': 'steelblue'},\n    pd_line_kw={'color': 'red', 'linewidth': 2},\n    n_jobs=-1\n)\nplt.suptitle('ICE + PDP', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SHAP vs LIME Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare explanations for the same prediction\n",
    "sample_idx = 0\n",
    "\n",
    "shap_exp = pd.Series(shap_values[sample_idx], index=X_test.columns, name='SHAP')\n",
    "\n",
    "lime_exp = lime_explainer.explain_instance(X_test.iloc[sample_idx].values, gb.predict, num_features=8)\n",
    "lime_dict = {feat.split(' ')[0]: w for feat, w in lime_exp.as_list()}\n",
    "lime_series = pd.Series(lime_dict, name='LIME')\n",
    "\n",
    "comparison = pd.DataFrame({'SHAP': shap_exp, 'LIME': lime_series}).fillna(0)\n",
    "comparison.plot(kind='barh', figsize=(10, 6))\n",
    "plt.title('SHAP vs LIME')\n",
    "plt.xlabel('Feature Contribution')\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}