{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. 선형 회귀 (Linear Regression)\n",
    "\n",
    "## 학습 목표\n",
    "- 선형 회귀의 원리 이해\n",
    "- scikit-learn으로 모델 구현\n",
    "- 모델 평가 지표 (MSE, R²) 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# 한글 폰트 설정 (선택)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인공 데이터 생성\n",
    "np.random.seed(42)\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=15, random_state=42)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.7, edgecolors='black')\n",
    "plt.xlabel('Feature (X)')\n",
    "plt.ylabel('Target (y)')\n",
    "plt.title('Linear Regression Data')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: 80:20 split is a common default — enough test data for reliable evaluation\n# while retaining most data for training. random_state ensures reproducibility.\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(f\"Train set: {X_train.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 회귀 모델 생성 및 학습\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 모델 파라미터 확인\n",
    "print(f\"기울기 (Coefficient): {model.coef_[0]:.4f}\")\n",
    "print(f\"절편 (Intercept): {model.intercept_:.4f}\")\n",
    "print(f\"\\n회귀식: y = {model.coef_[0]:.4f} * x + {model.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 예측 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 예측\ny_pred = model.predict(X_test)\n\n# Why: MSE penalizes large errors quadratically, making it sensitive to outliers.\n# RMSE converts back to original units for interpretability.\n# R² tells what fraction of variance is explained — the closer to 1, the better.\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"=== 모델 평가 ===\")\nprint(f\"MSE (Mean Squared Error): {mse:.4f}\")\nprint(f\"RMSE (Root MSE): {rmse:.4f}\")\nprint(f\"R² Score: {r2:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 결과 시각화\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 회귀선\naxes[0].scatter(X_test, y_test, alpha=0.7, label='Actual', edgecolors='black')\naxes[0].plot(X_test, y_pred, color='red', linewidth=2, label='Predicted')\naxes[0].set_xlabel('Feature (X)')\naxes[0].set_ylabel('Target (y)')\naxes[0].set_title('Linear Regression - Test Data')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Why: Residuals should be randomly scattered around zero. Any pattern (funnel,\n# curve) indicates violated assumptions like heteroscedasticity or non-linearity.\nresiduals = y_test - y_pred\naxes[1].scatter(y_pred, residuals, alpha=0.7, edgecolors='black')\naxes[1].axhline(y=0, color='red', linestyle='--')\naxes[1].set_xlabel('Predicted Values')\naxes[1].set_ylabel('Residuals')\naxes[1].set_title('Residual Plot')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 다중 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 특성 데이터 생성\n",
    "X_multi, y_multi = make_regression(\n",
    "    n_samples=200, \n",
    "    n_features=3, \n",
    "    noise=10, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# DataFrame으로 변환\n",
    "df = pd.DataFrame(X_multi, columns=['Feature_1', 'Feature_2', 'Feature_3'])\n",
    "df['Target'] = y_multi\n",
    "print(df.head())\n",
    "print(f\"\\nShape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 회귀 모델 학습\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_multi, y_multi, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_multi = LinearRegression()\n",
    "model_multi.fit(X_train_m, y_train_m)\n",
    "\n",
    "# 결과\n",
    "print(\"=== 다중 선형 회귀 ===\")\n",
    "print(f\"Coefficients: {model_multi.coef_}\")\n",
    "print(f\"Intercept: {model_multi.intercept_:.4f}\")\n",
    "\n",
    "y_pred_m = model_multi.predict(X_test_m)\n",
    "print(f\"\\nR² Score: {r2_score(y_test_m, y_pred_m):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 실제 데이터 예제 (Boston Housing 대체)\n",
    "\n",
    "sklearn의 Boston Housing 데이터셋은 deprecated되었으므로 California Housing을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# 데이터 로드\n",
    "housing = fetch_california_housing()\n",
    "X_housing = housing.data\n",
    "y_housing = housing.target\n",
    "\n",
    "print(f\"Features: {housing.feature_names}\")\n",
    "print(f\"Shape: {X_housing.shape}\")\n",
    "print(f\"Target: Median house value (in $100,000s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할 및 학습\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_housing = LinearRegression()\n",
    "model_housing.fit(X_train_h, y_train_h)\n",
    "\n",
    "y_pred_h = model_housing.predict(X_test_h)\n",
    "\n",
    "print(\"=== California Housing 회귀 결과 ===\")\n",
    "print(f\"R² Score: {r2_score(y_test_h, y_pred_h):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test_h, y_pred_h)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: Linear regression coefficients show each feature's marginal effect on the\n# target, but only meaningful when features are on comparable scales. Large\n# coefficients on unscaled data may just reflect unit differences, not importance.\nimportance = pd.DataFrame({\n    'Feature': housing.feature_names,\n    'Coefficient': model_housing.coef_\n}).sort_values('Coefficient', key=abs, ascending=True)\n\nplt.figure(figsize=(10, 6))\nplt.barh(importance['Feature'], importance['Coefficient'])\nplt.xlabel('Coefficient')\nplt.title('Feature Coefficients - California Housing')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "### 핵심 개념\n",
    "- **선형 회귀**: y = wx + b 형태의 선형 관계 학습\n",
    "- **경사 하강법**: 손실 함수(MSE)를 최소화하는 방향으로 파라미터 업데이트\n",
    "- **R² Score**: 모델이 데이터의 분산을 얼마나 설명하는지 (0~1, 높을수록 좋음)\n",
    "\n",
    "### 다음 단계\n",
    "- 다항 회귀 (Polynomial Regression)\n",
    "- 정규화 (Ridge, Lasso)\n",
    "- 특성 스케일링의 중요성"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}