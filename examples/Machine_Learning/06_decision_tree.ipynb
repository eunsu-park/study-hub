{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. 결정 트리 (Decision Tree)\n",
    "\n",
    "## 학습 목표\n",
    "- 결정 트리의 작동 원리 이해\n",
    "- 정보 이득, 지니 불순도 개념\n",
    "- 과적합 방지 (가지치기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error\n",
    "from sklearn.datasets import load_iris, make_classification\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 결정 트리 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris 데이터셋\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 2개 특성만 사용 (시각화 용이)\n",
    "X_2d = X[:, [2, 3]]  # petal length, petal width\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_2d, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: max_depth=3 limits tree complexity to prevent overfitting — deeper trees\n# memorize training data. Decision trees are greedy (locally optimal splits),\n# so constraining depth is the primary regularization mechanism.\ntree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree_clf.fit(X_train, y_train)\n\ny_pred = tree_clf.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=iris.target_names))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 트리 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결정 트리 시각화\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(tree_clf, \n",
    "          feature_names=['petal length', 'petal width'],\n",
    "          class_names=iris.target_names,\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=12)\n",
    "plt.title('Decision Tree - Iris Dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결정 경계 시각화\n",
    "def plot_decision_boundary_tree(model, X, y, feature_names, class_names):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "    \n",
    "    colors = ['blue', 'green', 'red']\n",
    "    for i, (color, name) in enumerate(zip(colors, class_names)):\n",
    "        idx = y == i\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=name, \n",
    "                    edgecolors='black', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel(feature_names[0])\n",
    "    plt.ylabel(feature_names[1])\n",
    "    plt.title('Decision Tree Decision Boundary')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary_tree(tree_clf, X_2d, y, \n",
    "                            ['petal length', 'petal width'],\n",
    "                            iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 과적합 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Why: This plot reveals the bias-variance tradeoff. Train score approaches 1.0\n# as depth increases (memorization), while test score peaks and then degrades.\n# The optimal depth is where test score is maximized — the sweet spot.\ndepths = range(1, 20)\ntrain_scores = []\ntest_scores = []\n\nfor depth in depths:\n    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n    tree.fit(X_train, y_train)\n    train_scores.append(tree.score(X_train, y_train))\n    test_scores.append(tree.score(X_test, y_test))\n\nplt.figure(figsize=(10, 6))\nplt.plot(depths, train_scores, 'b-o', label='Train Score')\nplt.plot(depths, test_scores, 'r-o', label='Test Score')\nplt.xlabel('Max Depth')\nplt.ylabel('Accuracy')\nplt.title('Decision Tree: Train vs Test Score by Depth')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 특성 중요도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 특성 사용 모델\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "tree_full = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "tree_full.fit(X_train_full, y_train_full)\n",
    "\n",
    "# 특성 중요도\n",
    "importance = pd.DataFrame({\n",
    "    'Feature': iris.feature_names,\n",
    "    'Importance': tree_full.feature_importances_\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance['Feature'], importance['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Decision Tree Feature Importance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.model_selection import GridSearchCV\n\n# Why: min_samples_split and min_samples_leaf complement max_depth for pruning.\n# min_samples_split prevents splits on tiny subsets, min_samples_leaf ensures\n# each leaf has enough data for a reliable prediction. Tuning all three together\n# via CV finds the best regularization combination.\nparam_grid = {\n    'max_depth': [2, 3, 4, 5, 6, 7, 8],\n    'min_samples_split': [2, 5, 10, 20],\n    'min_samples_leaf': [1, 2, 4, 8]\n}\n\ngrid_search = GridSearchCV(\n    DecisionTreeClassifier(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    return_train_score=True\n)\n\ngrid_search.fit(X_train_full, y_train_full)\n\nprint(f\"Best Parameters: {grid_search.best_params_}\")\nprint(f\"Best CV Score: {grid_search.best_score_:.4f}\")\nprint(f\"Test Score: {grid_search.score(X_test_full, y_test_full):.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 결정 트리 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회귀 데이터 생성\n",
    "np.random.seed(42)\n",
    "X_reg = np.sort(5 * np.random.rand(200, 1), axis=0)\n",
    "y_reg = np.sin(X_reg).ravel() + np.random.randn(200) * 0.1\n",
    "\n",
    "# 모델 학습\n",
    "tree_reg = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
    "tree_reg.fit(X_reg, y_reg)\n",
    "\n",
    "# 예측\n",
    "X_test_reg = np.linspace(0, 5, 500).reshape(-1, 1)\n",
    "y_pred_reg = tree_reg.predict(X_test_reg)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.scatter(X_reg, y_reg, alpha=0.5, label='Data')\n",
    "plt.plot(X_test_reg, y_pred_reg, 'r-', linewidth=2, label='Prediction')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Decision Tree Regression')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "### 핵심 개념\n",
    "- **분할 기준**: 정보 이득 (엔트로피) 또는 지니 불순도\n",
    "- **가지치기**: max_depth, min_samples_split, min_samples_leaf\n",
    "- **장점**: 해석 용이, 전처리 불필요\n",
    "- **단점**: 과적합 경향, 작은 변화에 민감\n",
    "\n",
    "### 다음 단계\n",
    "- 앙상블 학습 (Random Forest, Gradient Boosting)\n",
    "- 교차 검증을 통한 하이퍼파라미터 최적화"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}