# 01. NLP ê¸°ì´ˆ

## í•™ìŠµ ëª©í‘œ

- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê¸°ë²•
- í† í°í™” ë°©ë²• ì´í•´
- ì–´íœ˜ êµ¬ì¶•ê³¼ ì¸ì½”ë”©
- í…ìŠ¤íŠ¸ ì •ê·œí™”

---

## 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬

### ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

```
ì›ë³¸ í…ìŠ¤íŠ¸
    â†“
ì •ê·œí™” (ì†Œë¬¸ì, íŠ¹ìˆ˜ë¬¸ì ì œê±°)
    â†“
í† í°í™” (ë‹¨ì–´/ì„œë¸Œì›Œë“œ ë¶„ë¦¬)
    â†“
ë¶ˆìš©ì–´ ì œê±° (ì„ íƒ)
    â†“
ì–´íœ˜ êµ¬ì¶•
    â†“
ì¸ì½”ë”© (í…ìŠ¤íŠ¸ â†’ ìˆ«ì)
```

### ê¸°ë³¸ ì „ì²˜ë¦¬

```python
import re

def preprocess(text):
    # ì†Œë¬¸ì ë³€í™˜
    text = text.lower()

    # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'[^\w\s]', '', text)

    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r'\s+', ' ', text).strip()

    return text

text = "Hello, World! This is NLP   processing."
print(preprocess(text))
# "hello world this is nlp processing"
```

---

## 2. í† í°í™” (Tokenization)

### ë‹¨ì–´ í† í°í™”

```python
# ê³µë°± ê¸°ë°˜
text = "I love natural language processing"
tokens = text.split()
# ['I', 'love', 'natural', 'language', 'processing']

# NLTK
import nltk
from nltk.tokenize import word_tokenize
tokens = word_tokenize("I don't like it.")
# ['I', 'do', "n't", 'like', 'it', '.']
```

### ì„œë¸Œì›Œë“œ í† í°í™”

ì„œë¸Œì›Œë“œëŠ” ë‹¨ì–´ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„ë¦¬

```
"unhappiness" â†’ ["un", "##happiness"] (WordPiece)
"unhappiness" â†’ ["un", "happi", "ness"] (BPE)
```

**ì¥ì **:
- ë¯¸ë“±ë¡ ë‹¨ì–´(OOV) ì²˜ë¦¬ ê°€ëŠ¥
- ì–´íœ˜ í¬ê¸° ì¶•ì†Œ
- í˜•íƒœì†Œ ì •ë³´ ë³´ì¡´

### BPE (Byte Pair Encoding)

```python
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

# BPE í† í¬ë‚˜ì´ì € ìƒì„±
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
tokenizer.pre_tokenizer = Whitespace()

# í•™ìŠµ
trainer = BpeTrainer(special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]"])
tokenizer.train(files=["corpus.txt"], trainer=trainer)

# í† í°í™”
output = tokenizer.encode("Hello, world!")
print(output.tokens)
```

### WordPiece (BERT)

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

text = "I love natural language processing"
tokens = tokenizer.tokenize(text)
# ['i', 'love', 'natural', 'language', 'processing']

# ì¸ì½”ë”©
encoded = tokenizer.encode(text)
# [101, 1045, 2293, 3019, 2653, 6364, 102]

# ë””ì½”ë”©
decoded = tokenizer.decode(encoded)
# "[CLS] i love natural language processing [SEP]"
```

### SentencePiece (GPT, T5)

```python
import sentencepiece as spm

# í•™ìŠµ
spm.SentencePieceTrainer.train(
    input='corpus.txt',
    model_prefix='spm',
    vocab_size=8000,
    model_type='bpe'
)

# ë¡œë“œ ë° ì‚¬ìš©
sp = spm.SentencePieceProcessor()
sp.load('spm.model')

tokens = sp.encode_as_pieces("Hello, world!")
# ['â–Hello', ',', 'â–world', '!']

ids = sp.encode_as_ids("Hello, world!")
# [1234, 567, 890, 12]
```

---

## 3. ì–´íœ˜ êµ¬ì¶• (Vocabulary)

### ê¸°ë³¸ ì–´íœ˜ ì‚¬ì „

```python
from collections import Counter

class Vocabulary:
    def __init__(self, min_freq=1):
        self.word2idx = {'<pad>': 0, '<unk>': 1, '<bos>': 2, '<eos>': 3}
        self.idx2word = {0: '<pad>', 1: '<unk>', 2: '<bos>', 3: '<eos>'}
        self.word_freq = Counter()
        self.min_freq = min_freq

    def build(self, texts, tokenizer):
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        for text in texts:
            tokens = tokenizer(text)
            self.word_freq.update(tokens)

        # ë¹ˆë„ ê¸°ì¤€ í•„í„°ë§ í›„ ì¶”ê°€
        idx = len(self.word2idx)
        for word, freq in self.word_freq.items():
            if freq >= self.min_freq and word not in self.word2idx:
                self.word2idx[word] = idx
                self.idx2word[idx] = word
                idx += 1

    def encode(self, text, tokenizer):
        tokens = tokenizer(text)
        return [self.word2idx.get(t, self.word2idx['<unk>']) for t in tokens]

    def decode(self, indices):
        return [self.idx2word.get(i, '<unk>') for i in indices]

    def __len__(self):
        return len(self.word2idx)

# ì‚¬ìš©
vocab = Vocabulary(min_freq=2)
vocab.build(texts, str.split)
encoded = vocab.encode("hello world", str.split)
```

### torchtext ì–´íœ˜

```python
from torchtext.vocab import build_vocab_from_iterator

def yield_tokens(data_iter, tokenizer):
    for text in data_iter:
        yield tokenizer(text)

vocab = build_vocab_from_iterator(
    yield_tokens(texts, tokenizer),
    specials=['<pad>', '<unk>'],
    min_freq=2
)
vocab.set_default_index(vocab['<unk>'])

# ì‚¬ìš©
indices = vocab(tokenizer("hello world"))
```

---

## 4. íŒ¨ë”©ê³¼ ë°°ì¹˜ ì²˜ë¦¬

### ì‹œí€€ìŠ¤ íŒ¨ë”©

```python
import torch
from torch.nn.utils.rnn import pad_sequence

def collate_fn(batch):
    texts, labels = zip(*batch)

    # í† í°í™” ë° ì¸ì½”ë”©
    encoded = [torch.tensor(vocab.encode(t, tokenizer)) for t in texts]

    # íŒ¨ë”© (ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ì— ë§ì¶¤)
    padded = pad_sequence(encoded, batch_first=True, padding_value=0)

    # ìµœëŒ€ ê¸¸ì´ ì œí•œ
    if padded.size(1) > max_len:
        padded = padded[:, :max_len]

    labels = torch.tensor(labels)
    return padded, labels

# DataLoaderì— ì ìš©
from torch.utils.data import DataLoader
loader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)
```

### Attention Mask

```python
def create_attention_mask(input_ids, pad_token_id=0):
    """íŒ¨ë”©ì´ ì•„ë‹Œ ìœ„ì¹˜ëŠ” 1, íŒ¨ë”©ì€ 0"""
    return (input_ids != pad_token_id).long()

# ì˜ˆì‹œ
input_ids = torch.tensor([[1, 2, 3, 0, 0], [4, 5, 0, 0, 0]])
attention_mask = create_attention_mask(input_ids)
# tensor([[1, 1, 1, 0, 0], [1, 1, 0, 0, 0]])
```

---

## 5. í…ìŠ¤íŠ¸ ì •ê·œí™”

### ë‹¤ì–‘í•œ ì •ê·œí™” ê¸°ë²•

```python
import unicodedata

def normalize_text(text):
    # Unicode ì •ê·œí™” (NFD â†’ NFC)
    text = unicodedata.normalize('NFC', text)

    # ì†Œë¬¸ì ë³€í™˜
    text = text.lower()

    # URL ì œê±°
    text = re.sub(r'http\S+', '', text)

    # ì´ë©”ì¼ ì œê±°
    text = re.sub(r'\S+@\S+', '', text)

    # ìˆ«ì ì •ê·œí™” (ì„ íƒ)
    text = re.sub(r'\d+', '<NUM>', text)

    # ë°˜ë³µ ë¬¸ì ì¶•ì†Œ
    text = re.sub(r'(.)\1{2,}', r'\1\1', text)  # "sooooo" â†’ "soo"

    return text.strip()
```

### ë¶ˆìš©ì–´ ì œê±°

```python
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def remove_stopwords(tokens):
    return [t for t in tokens if t.lower() not in stop_words]

tokens = ['this', 'is', 'a', 'test', 'sentence']
filtered = remove_stopwords(tokens)
# ['test', 'sentence']
```

### í‘œì œì–´ ì¶”ì¶œ (Lemmatization)

```python
from nltk.stem import WordNetLemmatizer
import nltk

nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()

words = ['running', 'runs', 'ran', 'better', 'cats']
lemmas = [lemmatizer.lemmatize(w) for w in words]
# ['running', 'run', 'ran', 'better', 'cat']
```

---

## 6. HuggingFace í† í¬ë‚˜ì´ì €

### ê¸°ë³¸ ì‚¬ìš©

```python
from transformers import AutoTokenizer

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# ì¸ì½”ë”©
text = "Hello, how are you?"
encoded = tokenizer(
    text,
    padding='max_length',
    truncation=True,
    max_length=32,
    return_tensors='pt'
)

print(encoded['input_ids'].shape)      # torch.Size([1, 32])
print(encoded['attention_mask'].shape) # torch.Size([1, 32])
```

### ë°°ì¹˜ ì¸ì½”ë”©

```python
texts = ["Hello world", "NLP is fun", "I love Python"]

encoded = tokenizer(
    texts,
    padding=True,
    truncation=True,
    max_length=16,
    return_tensors='pt'
)

print(encoded['input_ids'].shape)  # torch.Size([3, 16])
```

### íŠ¹ìˆ˜ í† í°

```python
# BERT íŠ¹ìˆ˜ í† í°
print(tokenizer.special_tokens_map)
# {'unk_token': '[UNK]', 'sep_token': '[SEP]',
#  'pad_token': '[PAD]', 'cls_token': '[CLS]',
#  'mask_token': '[MASK]'}

# í† í° ID
print(tokenizer.cls_token_id)  # 101
print(tokenizer.sep_token_id)  # 102
print(tokenizer.pad_token_id)  # 0
```

---

## 7. ì‹¤ìŠµ: í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì „ì²˜ë¦¬

```python
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

class TextClassificationDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'label': torch.tensor(label)
        }

# ì‚¬ìš©
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
dataset = TextClassificationDataset(texts, labels, tokenizer)
loader = DataLoader(dataset, batch_size=32, shuffle=True)

for batch in loader:
    input_ids = batch['input_ids']       # (32, 128)
    attention_mask = batch['attention_mask']  # (32, 128)
    labels = batch['label']              # (32,)
    break
```

---

## ì •ë¦¬

### í† í°í™” ë°©ë²• ë¹„êµ

| ë°©ë²• | ì¥ì  | ë‹¨ì  | ì‚¬ìš© ëª¨ë¸ |
|------|------|------|----------|
| ë‹¨ì–´ ë‹¨ìœ„ | ì§ê´€ì  | OOV ë¬¸ì œ | ì „í†µ NLP |
| BPE | OOV í•´ê²° | í•™ìŠµ í•„ìš” | GPT |
| WordPiece | OOV í•´ê²° | í•™ìŠµ í•„ìš” | BERT |
| SentencePiece | ì–¸ì–´ ë¬´ê´€ | í•™ìŠµ í•„ìš” | T5, GPT |

### í•µì‹¬ ì½”ë“œ

```python
# HuggingFace í† í¬ë‚˜ì´ì €
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
encoded = tokenizer(text, padding=True, truncation=True, return_tensors='pt')

# ì–´íœ˜ êµ¬ì¶•
vocab = build_vocab_from_iterator(yield_tokens(texts), specials=['<pad>', '<unk>'])

# íŒ¨ë”©
padded = pad_sequence(sequences, batch_first=True, padding_value=0)
```

---

## ì—°ìŠµ ë¬¸ì œ

### ì—°ìŠµ ë¬¸ì œ 1: í† í°í™”(Tokenization) ë¹„êµ

ë¬¸ì¥ `"unhappiness is not the opposite of happiness"`ë¥¼ ì„¸ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ í† í°í™”í•˜ì„¸ìš”: (1) ë‹¨ìˆœ ê³µë°± ë¶„ë¦¬, (2) BERT WordPiece í† í¬ë‚˜ì´ì €, (3) HuggingFaceë¥¼ í†µí•œ GPT ìŠ¤íƒ€ì¼ BPE(Byte Pair Encoding). ê²°ê³¼ í† í°ì„ ë¹„êµí•˜ê³ , ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì €ê°€ íŠ¹ì • ë‹¨ì–´ë¥¼ ë‹¤ë¥´ê²Œ ë¶„ë¦¬í•˜ëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.

<details>
<summary>ì •ë‹µ ë³´ê¸°</summary>

```python
from transformers import BertTokenizer, GPT2Tokenizer

sentence = "unhappiness is not the opposite of happiness"

# 1. ê³µë°± ë¶„ë¦¬
whitespace_tokens = sentence.split()
print("ê³µë°± ë¶„ë¦¬:", whitespace_tokens)
# ['unhappiness', 'is', 'not', 'the', 'opposite', 'of', 'happiness']

# 2. BERT WordPiece
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_tokens = bert_tokenizer.tokenize(sentence)
print("BERT WordPiece:", bert_tokens)
# ['un', '##happiness', 'is', 'not', 'the', 'opposite', 'of', 'happiness']

# 3. GPT-2 BPE
gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
gpt2_tokens = gpt2_tokenizer.tokenize(sentence)
print("GPT-2 BPE:", gpt2_tokens)
# ['un', 'happiness', 'Ä is', 'Ä not', 'Ä the', 'Ä opposite', 'Ä of', 'Ä happiness']
```

**í•µì‹¬ ê´€ì°°**:
- ê³µë°± ë¶„ë¦¬ëŠ” ë³µí•©ì–´ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì—¬, "unhappiness"ê°€ í•™ìŠµ ë°ì´í„°ì— ì—†ì„ ê²½ìš° ë¯¸ë“±ë¡ ë‹¨ì–´(OOV) ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.
- BERTëŠ” ë‹¨ì–´ì˜ ì—°ì†ì„ í‘œì‹œí•˜ê¸° ìœ„í•´ `##` ì ‘ë‘ì‚¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (ì˜ˆ: `un` ë’¤ì— `##happiness`).
- GPT-2ëŠ” ë‹¨ì–´ì˜ ì‹œì‘ì„ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ `Ä ` (ê³µë°± ë§ˆì»¤)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ â€” ë¬¸ì¥ ì‹œì‘ì´ ì•„ë‹Œ ë‹¨ì–´ëŠ” ê³µë°± ì ‘ë‘ì‚¬ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
- ë‘ ì„œë¸Œì›Œë“œ ë°©ì‹ ëª¨ë‘ í•™ìŠµ ë°ì´í„°ì—ì„œ í¬ê·€í–ˆë”ë¼ë„ "un"ê³¼ "happiness"ë¼ëŠ” ì•Œë ¤ì§„ ì„œë¸Œì›Œë“œë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ "unhappiness"ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

</details>

### ì—°ìŠµ ë¬¸ì œ 2: ì–´í…ì…˜ ë§ˆìŠ¤í¬(Attention Mask) êµ¬ì„±

ê¸¸ì´ê°€ ë‹¤ë¥¸ ì„¸ ë¬¸ì¥ì´ í† í°í™”ëœ ë°°ì¹˜ê°€ ì£¼ì–´ì§ˆ ë•Œ, ë™ì¼í•œ ê¸¸ì´ë¡œ íŒ¨ë”©í•˜ê³  í•´ë‹¹ ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”. ì–´í…ì…˜ ë§ˆìŠ¤í¬ê°€ ì‹¤ì œ í† í°ì„ 1ë¡œ, íŒ¨ë”© í† í°ì„ 0ìœ¼ë¡œ ì˜¬ë°”ë¥´ê²Œ í‘œì‹œí•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

<details>
<summary>ì •ë‹µ ë³´ê¸°</summary>

```python
import torch
from torch.nn.utils.rnn import pad_sequence

# ì‹œë®¬ë ˆì´ì…˜ëœ í† í°í™” ì‹œí€€ìŠ¤ (ì´ë¯¸ IDë¡œ ì¸ì½”ë”©ë¨)
sequences = [
    torch.tensor([101, 7592, 2088, 102]),          # ê¸¸ì´ 4
    torch.tensor([101, 1045, 2293, 3019, 102]),    # ê¸¸ì´ 5
    torch.tensor([101, 4937, 102]),                 # ê¸¸ì´ 3
]

# ìµœëŒ€ ê¸¸ì´ë¡œ íŒ¨ë”© (pad_token_id = 0)
padded = pad_sequence(sequences, batch_first=True, padding_value=0)
print("íŒ¨ë”©ëœ input_ids:")
print(padded)
# tensor([[ 101, 7592, 2088,  102,    0],
#         [ 101, 1045, 2293, 3019,  102],
#         [ 101, 4937,  102,    0,    0]])

# ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±: ì‹¤ì œ í† í°ì€ 1, íŒ¨ë”©ì€ 0
attention_mask = (padded != 0).long()
print("\nì–´í…ì…˜ ë§ˆìŠ¤í¬:")
print(attention_mask)
# tensor([[1, 1, 1, 1, 0],
#         [1, 1, 1, 1, 1],
#         [1, 1, 1, 0, 0]])
```

ì–´í…ì…˜ ë§ˆìŠ¤í¬ëŠ” ì…€í”„ ì–´í…ì…˜(self-attention) ê³„ì‚° ì¤‘ ëª¨ë¸ì´ íŒ¨ë”© ìœ„ì¹˜ë¥¼ ë¬´ì‹œí•˜ë„ë¡ í•˜ì—¬, íŒ¨ë”© í† í°ì´ ì‹¤ì œ í† í°ì˜ í‘œí˜„ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.

</details>

### ì—°ìŠµ ë¬¸ì œ 3: ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ê³„

ì†Œì…œ ë¯¸ë””ì–´ ë°ì´í„°(íŠ¸ìœ—)ì— ëŒ€í•œ ê°ì„± ë¶„ì„ íƒœìŠ¤í¬ë¥¼ ìœ„í•œ ì™„ì „í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì„¤ê³„í•˜ì„¸ìš”. íŒŒì´í”„ë¼ì¸ì€ URL, í•´ì‹œíƒœê·¸, ë©˜ì…˜, ì´ëª¨ì§€, ë°˜ë³µ ë¬¸ìë¥¼ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. Python ì½”ë“œë¥¼ ì‘ì„±í•˜ê³  ê° ë‹¨ê³„ì˜ ëª©ì ì„ ì„¤ëª…í•˜ì„¸ìš”.

<details>
<summary>ì •ë‹µ ë³´ê¸°</summary>

```python
import re
import unicodedata

def preprocess_tweet(text):
    """
    ì†Œì…œ ë¯¸ë””ì–´ í…ìŠ¤íŠ¸(íŠ¸ìœ—) ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸.
    ê° ë‹¨ê³„ëŠ” íŠ¸ìœ— ë°ì´í„°ì˜ íŠ¹ì • ë…¸ì´ì¦ˆ ì›ì¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    # 1ë‹¨ê³„: ìœ ë‹ˆì½”ë“œ(Unicode) ì •ê·œí™” - ì•…ì„¼íŠ¸ ë¬¸ìë¥¼ ì¼ê´€ë˜ê²Œ ì²˜ë¦¬
    text = unicodedata.normalize('NFC', text)

    # 2ë‹¨ê³„: URL ì œê±° - URLì€ ê°ì„± ë¶„ì„ì— ê±°ì˜ ì˜ë¯¸ ì—†ëŠ” ì •ë³´ë¥¼ ë‹´ê³  ìˆìŒ
    text = re.sub(r'http\S+|www\S+', '', text)

    # 3ë‹¨ê³„: ë©˜ì…˜ì„ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ êµì²´ - ì†Œì…œ ì‹ í˜¸ëŠ” ë³´ì¡´í•˜ë˜ íŠ¹ì • ì‚¬ìš©ìëª… ê³¼ì í•© ë°©ì§€
    text = re.sub(r'@\w+', '@user', text)

    # 4ë‹¨ê³„: í•´ì‹œíƒœê·¸ ë‚´ìš© ì¶”ì¶œ (# ì œê±°, ë‹¨ì–´ ìœ ì§€)
    text = re.sub(r'#(\w+)', r'\1', text)

    # 5ë‹¨ê³„: ì´ëª¨ì§€ ì œê±° - ì„ íƒ ì‚¬í•­; í…ìŠ¤íŠ¸ ì„¤ëª…ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ë„ ìˆìŒ
    text = text.encode('ascii', 'ignore').decode('ascii')

    # 6ë‹¨ê³„: ë°˜ë³µ ë¬¸ì ì¶•ì†Œ - "soooo good" â†’ "soo good"
    text = re.sub(r'(.)\1{2,}', r'\1\1', text)

    # 7ë‹¨ê³„: ì†Œë¬¸ì ë³€í™˜ ë° ì•ë’¤ ê³µë°± ì œê±°
    text = text.lower().strip()

    # 8ë‹¨ê³„: ê³µë°± ì •ê·œí™”
    text = re.sub(r'\s+', ' ', text)

    return text

# í…ŒìŠ¤íŠ¸
tweet = "OMG this is soooo amazing!! ğŸ˜ Check out https://example.com #NLP @anthropic"
print(preprocess_tweet(tweet))
# "omg this is soo amazing check out nlp @user"
```

**ì„¤ê³„ ê·¼ê±°**:
- URL ì œê±°: ê°ì„±ì„ ì „ë‹¬í•˜ì§€ ì•Šê³  ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
- ë©˜ì…˜ ì •ê·œí™”: íŠ¹ì • ì‚¬ìš©ìëª…ì— ê³¼ì í•©í•˜ì§€ ì•Šìœ¼ë©´ì„œ ì†Œì…œ ìƒí˜¸ì‘ìš© ì‹ í˜¸ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.
- ë°˜ë³µ ë¬¸ì ì¶•ì†Œ (ì œê±°í•˜ì§€ ì•ŠìŒ): "sooo"ëŠ” "ë§¤ìš°"ë¥¼ ì˜ë¯¸í•  ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ 2ê°œì˜ ë¬¸ìë¥¼ ìœ ì§€í•˜ì—¬ ê°•ì¡°ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
- URL íŒ¨í„´ ë§¤ì¹­ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡ URL ì œê±° í›„ì— ì†Œë¬¸ì ë³€í™˜ì„ ì ìš©í•©ë‹ˆë‹¤.

</details>

### ì—°ìŠµ ë¬¸ì œ 4: ì–´íœ˜ ë²”ìœ„(Vocabulary Coverage) ë¶„ì„

í›ˆë ¨ ì½”í¼ìŠ¤ì—ì„œ ì–´íœ˜ë¥¼ êµ¬ì¶•í•˜ê³ , ë‹¤ì–‘í•œ ì–´íœ˜ í¬ê¸°(1k, 5k, 10k, 50k ë‹¨ì–´)ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œì˜ ë¯¸ë“±ë¡ ë‹¨ì–´(OOV, Out-of-Vocabulary) ë¹„ìœ¨ì„ ë¶„ì„í•˜ì„¸ìš”. ê²°ê³¼ë¥¼ í…Œì´ë¸”ë¡œ ì •ë¦¬í•˜ê³ , ì–´íœ˜ í¬ê¸°ì™€ OOV ë¹„ìœ¨ ê°„ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.

<details>
<summary>ì •ë‹µ ë³´ê¸°</summary>

```python
from collections import Counter
import numpy as np

def analyze_vocabulary_coverage(train_texts, test_texts, tokenizer, vocab_sizes):
    """
    ë‹¤ì–‘í•œ ì–´íœ˜ í¬ê¸°ì— ëŒ€í•œ OOV ë¹„ìœ¨ ë¶„ì„.
    """
    # í›ˆë ¨ ì„¸íŠ¸ì˜ ëª¨ë“  ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
    train_counter = Counter()
    for text in train_texts:
        train_counter.update(tokenizer(text))

    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ëª¨ë“  í† í° ê³„ì‚°
    test_tokens = []
    for text in test_texts:
        test_tokens.extend(tokenizer(text))
    total_test_tokens = len(test_tokens)

    results = {}
    for vocab_size in vocab_sizes:
        # ìƒìœ„ kê°œ ë‹¨ì–´ë¡œ ì–´íœ˜ êµ¬ì¶•
        top_words = set(w for w, _ in train_counter.most_common(vocab_size))

        # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ OOV í† í° ê³„ì‚°
        oov_count = sum(1 for t in test_tokens if t not in top_words)
        oov_rate = oov_count / total_test_tokens * 100

        results[vocab_size] = {
            'oov_rate': oov_rate,
            'coverage': 100 - oov_rate
        }
        print(f"ì–´íœ˜ í¬ê¸° {vocab_size:6d}: OOV ë¹„ìœ¨ = {oov_rate:.2f}%, ë²”ìœ„ = {100-oov_rate:.2f}%")

    return results

# ì˜ˆì‹œ ì¶œë ¥ (ì¼ë°˜ì ì¸ ì˜ì–´ ì½”í¼ìŠ¤ì˜ ê·¼ì‚¬ê°’):
# ì–´íœ˜ í¬ê¸°   1000: OOV ë¹„ìœ¨ = 15.30%, ë²”ìœ„ = 84.70%
# ì–´íœ˜ í¬ê¸°   5000: OOV ë¹„ìœ¨ =  5.10%, ë²”ìœ„ = 94.90%
# ì–´íœ˜ í¬ê¸°  10000: OOV ë¹„ìœ¨ =  2.80%, ë²”ìœ„ = 97.20%
# ì–´íœ˜ í¬ê¸°  50000: OOV ë¹„ìœ¨ =  0.90%, ë²”ìœ„ = 99.10%
```

**íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„**:
- ì–´íœ˜ í¬ê¸°ê°€ í´ìˆ˜ë¡ OOV ë¹„ìœ¨ì€ ë‚®ì•„ì§€ì§€ë§Œ ì„ë² ë”© í–‰ë ¬ì´ ì»¤ì§‘ë‹ˆë‹¤ (ë©”ëª¨ë¦¬ ë¹„ìš©ì€ `vocab_size Ã— embed_dim`).
- BPE(Byte Pair Encoding), WordPieceì™€ ê°™ì€ ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì €ëŠ” ë¯¸ì§€ì˜ ë‹¨ì–´ë¥¼ ì•Œë ¤ì§„ ì„œë¸Œì›Œë“œë¡œ ë¶„í•´í•˜ì—¬ ì‘ì€ ì–´íœ˜(~30kâ€“50k í† í°)ë¡œ ê±°ì˜ 0%ì— ê°€ê¹Œìš´ OOV ë¹„ìœ¨ì„ ë‹¬ì„±í•©ë‹ˆë‹¤.
- ë‹¨ì–´ ìˆ˜ì¤€ ëª¨ë¸ì˜ ê²½ìš°, ë²”ìœ„ì™€ ë©”ëª¨ë¦¬ ê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•´ 30kâ€“50k ì–´íœ˜ê°€ ì¼ë°˜ì ì¸ ì‹¤ìš©ì  ì„ íƒì…ë‹ˆë‹¤.

</details>

### ì—°ìŠµ ë¬¸ì œ 5: í† í¬ë‚˜ì´ì € íŠ¹ìˆ˜ í† í°(Special Token) ì—­í• 

BERT í† í¬ë‚˜ì´ì €ì˜ íŠ¹ìˆ˜ í† í°ì¸ `[CLS]`, `[SEP]`, `[PAD]`, `[MASK]`, `[UNK]`ì˜ ëª©ì ì„ ì„¤ëª…í•˜ì„¸ìš”. ê° í† í°ì— ëŒ€í•´ HuggingFaceì˜ `BertTokenizer`ë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ IDì— ì ‘ê·¼í•˜ëŠ” ì½”ë“œë¥¼ í•œ ì¤„ì”© ì‘ì„±í•˜ì„¸ìš”.

<details>
<summary>ì •ë‹µ ë³´ê¸°</summary>

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# [CLS] - ë¶„ë¥˜(Classification) í† í°, ëª¨ë“  ì…ë ¥ì˜ ì•ì— ì¶”ê°€ë©ë‹ˆë‹¤.
#         ìµœì¢… ì€ë‹‰ ìƒíƒœê°€ ë¶„ë¥˜ íƒœìŠ¤í¬ì˜ ì‹œí€€ìŠ¤ í‘œí˜„ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
print(f"[CLS] ID: {tokenizer.cls_token_id}")   # 101

# [SEP] - êµ¬ë¶„ì(Separator) í† í°, ê° ì„¸ê·¸ë¨¼íŠ¸ ëì— ì¶”ê°€ë©ë‹ˆë‹¤.
#         NLIë‚˜ QA ê°™ì€ íƒœìŠ¤í¬ì—ì„œ ë¬¸ì¥ Aì™€ Bë¥¼ êµ¬ë¶„í•©ë‹ˆë‹¤.
print(f"[SEP] ID: {tokenizer.sep_token_id}")   # 102

# [PAD] - íŒ¨ë”©(Padding) í† í°, ì§§ì€ ì‹œí€€ìŠ¤ë¥¼ ë°°ì¹˜ ê¸¸ì´ì— ë§ê²Œ ì±„ì›ë‹ˆë‹¤.
#         ê³„ì‚°ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ í•­ìƒ mask=0ìœ¼ë¡œ ì–´í…ì…˜ë©ë‹ˆë‹¤.
print(f"[PAD] ID: {tokenizer.pad_token_id}")   # 0

# [MASK] - ë§ˆìŠ¤í‚¹(Masking) í† í°, MLM(Masked Language Modeling) ì‚¬ì „í•™ìŠµ ì¤‘ í† í°ì˜ 15%ë¥¼ ëŒ€ì²´í•©ë‹ˆë‹¤.
#          ëª¨ë¸ì€ ë¬¸ë§¥ìœ¼ë¡œë¶€í„° ì›ë˜ í† í°ì„ ì˜ˆì¸¡í•´ì•¼ í•©ë‹ˆë‹¤.
print(f"[MASK] ID: {tokenizer.mask_token_id}") # 103

# [UNK] - ë¯¸ì§€(Unknown) í† í°, í† í°í™”í•  ìˆ˜ ì—†ëŠ” ë‹¨ì–´ë¥¼ ëŒ€ì²´í•©ë‹ˆë‹¤.
#         WordPieceëŠ” ëŒ€ë¶€ë¶„ì˜ ë‹¨ì–´ë¥¼ ì„œë¸Œì›Œë“œë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ ê±°ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
print(f"[UNK] ID: {tokenizer.unk_token_id}")   # 100

# ëª¨ë“  í† í°ì´ ë³´ì´ëŠ” ë¬¸ì¥ ì¸ì½”ë”©ìœ¼ë¡œ í™•ì¸
encoded = tokenizer("Hello [MASK] world", return_tensors='pt')
print(tokenizer.convert_ids_to_tokens(encoded['input_ids'][0].tolist()))
# ['[CLS]', 'hello', '[MASK]', 'world', '[SEP]']
```

**ì—­í•  ìš”ì•½**:

| í† í° | ì—­í•  | ì‚¬ìš© ì‹œì  |
|------|------|-----------|
| `[CLS]` | ì‹œí€€ìŠ¤ ì „ì²´ í‘œí˜„ ì§‘ê³„ | ëª¨ë“  ì…ë ¥ì˜ ì‹œì‘ |
| `[SEP]` | ë¬¸ì¥ ê²½ê³„ ë§ˆì»¤ | ê° ë¬¸ì¥ ì„¸ê·¸ë¨¼íŠ¸ì˜ ë |
| `[PAD]` | ê³ ì • ê¸¸ì´ë¥¼ ìœ„í•œ ì±„ì›€ | ë°°ì¹˜ íŒ¨ë”© |
| `[MASK]` | MLM ì‚¬ì „í•™ìŠµ ëŒ€ìƒ | í•™ìŠµ ì¤‘ í† í°ì˜ 15% |
| `[UNK]` | ë¯¸ì§€ í† í° í´ë°± | í¬ê·€; ì„œë¸Œì›Œë“œê°€ ëŒ€ë¶€ë¶„ ì²˜ë¦¬ |

</details>

## ë‹¤ìŒ ë‹¨ê³„

[Word2Vecê³¼ GloVe](./02_Word2Vec_GloVe.md)ì—ì„œ ë‹¨ì–´ ì„ë² ë”©ì„ í•™ìŠµí•©ë‹ˆë‹¤.
