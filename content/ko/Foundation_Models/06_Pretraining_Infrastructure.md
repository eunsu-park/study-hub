# 06. Pre-training μΈν”„λΌ

## ν•™μµ λ©ν‘(Learning Objectives)

μ΄ λ μ¨μ„ μ™„λ£ν•λ©΄ λ‹¤μμ„ ν•  μ μμµλ‹λ‹¤:

1. λ„¤ κ°€μ§€ μ£Όμ” λ³‘λ ¬ν™” μ „λµ(λ°μ΄ν„°, ν…μ„, νμ΄ν”„λΌμΈ, μ‹ν€€μ¤)μ„ μ„¤λ…ν•κ³ , μμ²μ–µ κ° νλΌλ―Έν„°μ λ¨λΈ ν•™μµμ„ μ„ν•΄ 3D λ³‘λ ¬ μ²λ¦¬(3D Parallelism)μ—μ„ μ΄λ“¤μ΄ μ–΄λ–»κ² κ²°ν•©λλ”μ§€ μ„¤λ…ν•  μ μμµλ‹λ‹¤.
2. λ¨λΈ νλΌλ―Έν„°, κ·Έλλ””μ–ΈνΈ(Gradient), μµν‹°λ§μ΄μ € μƒνƒ(Optimizer States), ν™μ„±ν™”(Activation)λ¥Ό κ³ λ ¤ν•μ—¬ μ£Όμ–΄μ§„ λ¨λΈ ν¬κΈ°μ™€ λ°°μΉ κµ¬μ„±μ— λ€ν• GPU λ©”λ¨λ¦¬ μ”κµ¬μ‚¬ν•­μ„ μ¶”μ •ν•  μ μμµλ‹λ‹¤.
3. κ·Έλλ””μ–ΈνΈ μ²΄ν¬ν¬μΈν…(Gradient Checkpointing), ZeRO μµν‹°λ§μ΄μ € λ‹¨κ³„, νΌν•© μ •λ°€λ„(fp16/bf16) ν•™μµ, ν™μ„±ν™” μ¤ν”„λ΅λ”©(Activation Offloading)μ„ ν¬ν•¨ν• λ©”λ¨λ¦¬ μµμ ν™” κΈ°λ²•μ„ μ μ©ν•  μ μμµλ‹λ‹¤.
4. κ·Έλλ””μ–ΈνΈ ν΄λ¦¬ν•‘(Gradient Clipping), μ†μ‹¤ μ¤μΌ€μΌλ§(Loss Scaling), ν•™μµλ¥  μ›λ°μ—…(Learning Rate Warm-up) λ“±μ ν•™μµ μ•μ •μ„± κΈ°λ²•μ΄ μΌλ°μ μΈ ν•™μµ μ‹¤ν¨(μ†μ‹¤ κΈ‰μ¦, κ·Έλλ””μ–ΈνΈ ν­λ°)λ¥Ό λ°©μ§€ν•λ” λ°©λ²•μ„ μ„¤λ…ν•  μ μμµλ‹λ‹¤.
5. Megatron-LM, DeepSpeed, PyTorch FSDP λ“±μ ν”„λ μ„μ›ν¬λ¥Ό μ‚¬μ©ν•μ—¬ λ¶„μ‚° ν•™μµ ν™κ²½μ„ κµ¬μ„±ν•κ³ , μ£Όμ–΄μ§„ λ¨λΈ λ° ν•λ“μ›¨μ–΄ κµ¬μ„±μ— λ§λ” λ³‘λ ¬ν™” μ „λµ μ„ νƒμ„ μ •λ‹Ήν™”ν•  μ μμµλ‹λ‹¤.
6. μ²λ¦¬λ‰ μ§€ν‘(MFU, GPU ν™μ©λ¥ )λ¥Ό λ¶„μ„ν•κ³ , ν†µμ‹  μ¤λ²„ν—¤λ“, λ¶€ν• λ¶κ· ν•, λ©”λ¨λ¦¬ μ••λ°•μΌλ΅ μΈν• λ€κ·λ¨ ν•™μµ μ‹¤ν–‰μ λ³‘λ© ν„μƒμ„ μ‹λ³„ν•  μ μμµλ‹λ‹¤.

---

## κ°μ”

λ€κ·λ¨ Foundation Model ν•™μµμ€ μμ² κ°μ GPUμ—μ„ μμ£Όμ—μ„ μκ°μ›”κ°„ μ§„ν–‰λ©λ‹λ‹¤. μ΄ λ μ¨μ—μ„λ” λ¶„μ‚° ν•™μµ μ „λµ, λ©”λ¨λ¦¬ μµμ ν™”, ν•™μµ μ•μ •μ„± κΈ°λ²•μ„ λ‹¤λ£Ήλ‹λ‹¤.

---

## 1. λ¶„μ‚° ν•™μµ ν¨λ¬λ‹¤μ„

### 1.1 λ³‘λ ¬ν™” μ „λµ κ°μ”

```
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                     λ¶„μ‚° ν•™μµ ν¨λ¬λ‹¤μ„                            β”‚
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¤
β”‚                                                                  β”‚
β”‚  Data Parallelism (DP)         Tensor Parallelism (TP)          β”‚
β”‚  β”β”€β”€β”€β”€β”€β” β”β”€β”€β”€β”€β”€β”               β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”              β”‚
β”‚  β”‚GPU 0β”‚ β”‚GPU 1β”‚               β”‚   W = [W1 | W2]  β”‚              β”‚
β”‚  β”‚Modelβ”‚ β”‚Modelβ”‚               β”‚GPU0    GPU1      β”‚              β”‚
β”‚  β”‚Data1β”‚ β”‚Data2β”‚               β”‚ W1      W2       β”‚              β”‚
β”‚  β””β”€β”€β”€β”€β”€β” β””β”€β”€β”€β”€β”€β”               β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”              β”‚
β”‚  λ™μΌ λ¨λΈ, λ‹¤λ¥Έ λ°μ΄ν„°         λ μ΄μ–΄λ¥Ό GPUκ°„ λ¶„ν•                β”‚
β”‚                                                                  β”‚
β”‚  Pipeline Parallelism (PP)     Sequence Parallelism (SP)        β”‚
β”‚  β”β”€β”€β”€β”€β”€β” β”β”€β”€β”€β”€β”€β”               β”β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”¬β”€β”€β”€β”€β”             β”‚
β”‚  β”‚GPU 0β”‚ β”‚GPU 1β”‚               β”‚ S1 β”‚ S2 β”‚ S3 β”‚ S4 β”‚             β”‚
β”‚  β”‚L1-L6β”‚β†’β”‚L7-12β”‚               β”‚GPU0β”‚GPU1β”‚GPU2β”‚GPU3β”‚             β”‚
β”‚  β””β”€β”€β”€β”€β”€β” β””β”€β”€β”€β”€β”€β”               β””β”€β”€β”€β”€β”΄β”€β”€β”€β”€β”΄β”€β”€β”€β”€β”΄β”€β”€β”€β”€β”             β”‚
β”‚  λ μ΄μ–΄λ¥Ό μμ°¨ λ¶„ν•              μ‹ν€€μ¤λ¥Ό GPUκ°„ λ¶„ν•                β”‚
β”‚                                                                  β”‚
β”‚  3D Parallelism: DP + TP + PP μ΅°ν•©                              β”‚
β”‚                                                                  β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
```

### 1.2 λ©”λ¨λ¦¬ λ¶„μ„

```python
def estimate_training_memory(
    num_params: int,  # νλΌλ―Έν„° μ
    batch_size: int,
    seq_len: int,
    hidden_dim: int,
    num_layers: int,
    dtype_bytes: int = 2,  # fp16/bf16 = 2, fp32 = 4
    optimizer: str = 'adam'
) -> dict:
    """
    ν•™μµ μ‹ GPU λ©”λ¨λ¦¬ μ¶”μ •

    λ©”λ¨λ¦¬ κµ¬μ„±:
    1. Model Parameters
    2. Gradients
    3. Optimizer States
    4. Activations (forward pass)
    """

    # 1. λ¨λΈ νλΌλ―Έν„°
    param_memory = num_params * dtype_bytes

    # 2. Gradients (νλΌλ―Έν„°μ™€ λ™μΌ)
    grad_memory = num_params * dtype_bytes

    # 3. Optimizer States
    if optimizer == 'adam':
        # Adam: momentum(fp32) + variance(fp32)
        optimizer_memory = num_params * 4 * 2  # 8 bytes per param
    elif optimizer == 'sgd':
        optimizer_memory = num_params * 4  # momentum only
    else:
        optimizer_memory = 0

    # 4. Activations (κ·Όμ‚¬μΉ)
    # κ° λ μ΄μ–΄: attention + FFN activations
    bytes_per_token = hidden_dim * dtype_bytes * 10  # κ·Όμ‚¬
    activation_memory = batch_size * seq_len * bytes_per_token * num_layers

    # Activation checkpointing μ‹ 1/sqrt(L) λ΅ κ°μ†

    total = param_memory + grad_memory + optimizer_memory + activation_memory

    return {
        'parameters_gb': param_memory / 1e9,
        'gradients_gb': grad_memory / 1e9,
        'optimizer_gb': optimizer_memory / 1e9,
        'activations_gb': activation_memory / 1e9,
        'total_gb': total / 1e9
    }


# μμ‹: 7B λ¨λΈ
memory = estimate_training_memory(
    num_params=7e9,
    batch_size=4,
    seq_len=2048,
    hidden_dim=4096,
    num_layers=32
)

print("7B λ¨λΈ λ©”λ¨λ¦¬ μ¶”μ •:")
for key, value in memory.items():
    print(f"  {key}: {value:.1f} GB")

# μ¶λ ¥:
# parameters_gb: 14.0 GB
# gradients_gb: 14.0 GB
# optimizer_gb: 56.0 GB
# activations_gb: ~21.5 GB (batch_size=4)
# total_gb: ~105.5 GB
```

---

## 2. FSDP (Fully Sharded Data Parallel)

### 2.1 FSDP κ°λ…

```
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                      FSDP λ™μ‘ μ›λ¦¬                         β”‚
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¤
β”‚                                                             β”‚
β”‚  κΈ°μ΅΄ DDP:                                                  β”‚
β”‚  GPU 0: [Full Model] + [Data 0]                            β”‚
β”‚  GPU 1: [Full Model] + [Data 1]                            β”‚
β”‚  β†’ κ° GPUμ— μ „μ²΄ λ¨λΈ λ³µμ  (λΉ„ν¨μ¨)                          β”‚
β”‚                                                             β”‚
β”‚  FSDP (Zero Stage 3):                                       β”‚
β”‚  GPU 0: [Shard 0] + [Data 0]                               β”‚
β”‚  GPU 1: [Shard 1] + [Data 1]                               β”‚
β”‚                                                             β”‚
β”‚  Forward μ‹: All-Gatherλ΅ μ „μ²΄ νλΌλ―Έν„° μμ§‘                β”‚
β”‚  Backward μ‹: Reduce-Scatterλ΅ gradient λ¶„μ‚°               β”‚
β”‚                                                             β”‚
β”‚  λ©”λ¨λ¦¬: (Params + Grads + Optim) / N + Activations         β”‚
β”‚                                                             β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
```

### 2.2 PyTorch FSDP κµ¬ν„

```python
import torch
import torch.distributed as dist
from torch.distributed.fsdp import (
    FullyShardedDataParallel as FSDP,
    MixedPrecision,
    BackwardPrefetch,
    ShardingStrategy,
    CPUOffload,
)
from torch.distributed.fsdp.wrap import (
    transformer_auto_wrap_policy,
    size_based_auto_wrap_policy,
)
import functools

def setup_fsdp_training():
    """FSDP ν•™μµ μ„¤μ •"""

    # λ¶„μ‚° μ΄κΈ°ν™”
    dist.init_process_group("nccl")
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)

    # λ¨λΈ μƒμ„±
    model = MyTransformerModel(config)

    # Mixed Precision μ„¤μ •
    mixed_precision = MixedPrecision(
        param_dtype=torch.bfloat16,      # νλΌλ―Έν„°
        reduce_dtype=torch.bfloat16,     # gradient reduction
        buffer_dtype=torch.bfloat16,     # λ²„νΌ
    )

    # Auto Wrap Policy: Transformer λ μ΄μ–΄ λ‹¨μ„λ΅ μƒ¤λ”©
    wrap_policy = functools.partial(
        transformer_auto_wrap_policy,
        transformer_layer_cls={TransformerBlock},
    )

    # FSDP λν•‘
    model = FSDP(
        model,
        sharding_strategy=ShardingStrategy.FULL_SHARD,  # Zero-3
        mixed_precision=mixed_precision,
        auto_wrap_policy=wrap_policy,
        backward_prefetch=BackwardPrefetch.BACKWARD_PRE,
        cpu_offload=CPUOffload(offload_params=False),
        device_id=local_rank,
    )

    return model


def train_step_fsdp(model, batch, optimizer, scaler=None):
    """FSDP ν•™μµ μ¤ν…"""
    model.train()

    # Forward
    with torch.cuda.amp.autocast(dtype=torch.bfloat16):
        outputs = model(**batch)
        loss = outputs.loss

    # Backward
    loss.backward()

    # Gradient clipping (FSDPμ—μ„λ” μ£Όμ ν•„μ”)
    model.clip_grad_norm_(max_norm=1.0)

    # Optimizer step
    optimizer.step()
    optimizer.zero_grad()

    return loss.item()


# μ²΄ν¬ν¬μΈνΈ μ €μ¥/λ΅λ“
from torch.distributed.fsdp import (
    FullStateDictConfig,
    StateDictType,
)

def save_fsdp_checkpoint(model, optimizer, path):
    """FSDP μ²΄ν¬ν¬μΈνΈ μ €μ¥"""

    # Full State Dict μ„¤μ •
    full_state_dict_config = FullStateDictConfig(
        offload_to_cpu=True,
        rank0_only=True,
    )

    with FSDP.state_dict_type(
        model,
        StateDictType.FULL_STATE_DICT,
        full_state_dict_config,
    ):
        state_dict = model.state_dict()
        optim_state = FSDP.optim_state_dict(model, optimizer)

        if dist.get_rank() == 0:
            torch.save({
                'model': state_dict,
                'optimizer': optim_state,
            }, path)

    dist.barrier()
```

---

## 3. DeepSpeed ZeRO

### 3.1 ZeRO λ‹¨κ³„λ³„ λΉ„κµ

```
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚                     DeepSpeed ZeRO λ‹¨κ³„                    β”‚
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”¤
β”‚                                                            β”‚
β”‚  Stage 1: Optimizer State Partitioning                    β”‚
β”‚  - Optimizer states (Adam m, v)λ§ λ¶„ν•                     β”‚
β”‚  - λ©”λ¨λ¦¬ μ κ°: ~4x                                        β”‚
β”‚                                                            β”‚
β”‚  Stage 2: + Gradient Partitioning                         β”‚
β”‚  - Gradientsλ„ λ¶„ν•                                         β”‚
β”‚  - λ©”λ¨λ¦¬ μ κ°: ~8x                                        β”‚
β”‚                                                            β”‚
β”‚  Stage 3: + Parameter Partitioning                        β”‚
β”‚  - Parametersλ„ λ¶„ν•  (FSDPμ™€ μ μ‚¬)                         β”‚
β”‚  - λ©”λ¨λ¦¬ μ κ°: ~N (GPU μμ— λΉ„λ΅€)                         β”‚
β”‚                                                            β”‚
β”‚  ZeRO-Offload: CPU/NVMeλ΅ μ¤ν”„λ΅λ“                         β”‚
β”‚  ZeRO-Infinity: λ¬΄ν• λ¨λΈ ν¬κΈ° μ§€μ›                        β”‚
β”‚                                                            β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
```

### 3.2 DeepSpeed μ„¤μ •

```python
# ds_config.json
ds_config = {
    "train_batch_size": 256,
    "gradient_accumulation_steps": 8,
    "train_micro_batch_size_per_gpu": 4,

    # FP16 μ„¤μ •
    "fp16": {
        "enabled": True,
        "loss_scale": 0,  # dynamic
        "loss_scale_window": 1000,
        "initial_scale_power": 16,
        "hysteresis": 2,
        "min_loss_scale": 1
    },

    # BF16 μ„¤μ • (λ€μ•)
    "bf16": {
        "enabled": False
    },

    # ZeRO Stage 3
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": {
            "device": "cpu",  # or "nvme"
            "pin_memory": True
        },
        "offload_param": {
            "device": "cpu",
            "pin_memory": True
        },
        "overlap_comm": True,
        "contiguous_gradients": True,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": True,
    },

    # Gradient Checkpointing
    "activation_checkpointing": {
        "partition_activations": True,
        "cpu_checkpointing": True,
        "contiguous_memory_optimization": True,
        "number_checkpoints": None,
        "synchronize_checkpoint_boundary": False,
        "profile": False
    },

    # Optimizer
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 3e-4,
            "betas": [0.9, 0.999],
            "eps": 1e-8,
            "weight_decay": 0.01
        }
    },

    # Scheduler
    "scheduler": {
        "type": "WarmupDecayLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 3e-4,
            "warmup_num_steps": 1000,
            "total_num_steps": 100000
        }
    }
}
```

### 3.3 DeepSpeed ν•™μµ μ½”λ“

```python
import deepspeed
import torch

def train_with_deepspeed():
    """DeepSpeed ν•™μµ λ£¨ν”„"""

    # λ¨λΈ λ° λ°μ΄ν„°
    model = MyTransformerModel(config)
    train_dataloader = create_dataloader(...)

    # DeepSpeed μ΄κΈ°ν™”
    model_engine, optimizer, _, lr_scheduler = deepspeed.initialize(
        model=model,
        model_parameters=model.parameters(),
        config=ds_config,
    )

    # ν•™μµ λ£¨ν”„
    for epoch in range(num_epochs):
        for step, batch in enumerate(train_dataloader):
            batch = {k: v.to(model_engine.device) for k, v in batch.items()}

            # Forward
            outputs = model_engine(**batch)
            loss = outputs.loss

            # Backward (DeepSpeedκ°€ gradient scaling/accumulation μ²λ¦¬)
            model_engine.backward(loss)

            # Step
            model_engine.step()

            if step % 100 == 0:
                print(f"Step {step}, Loss: {loss.item():.4f}")

    # μ²΄ν¬ν¬μΈνΈ μ €μ¥
    model_engine.save_checkpoint("checkpoint_dir")


# μ‹¤ν–‰
# deepspeed --num_gpus=8 train.py --deepspeed_config ds_config.json
```

---

## 4. Activation Checkpointing (Gradient Checkpointing)

### 4.1 κ°λ…

```
μΌλ° Forward:
Layer 1 β†’ [Act1 μ €μ¥] β†’ Layer 2 β†’ [Act2 μ €μ¥] β†’ ... β†’ Loss

Backward μ‹ Act1, Act2 λ“±μ„ μ‚¬μ©ν•μ—¬ gradient κ³„μ‚°
β†’ λ©”λ¨λ¦¬: O(L) - λ μ΄μ–΄ μμ— λΉ„λ΅€

Activation Checkpointing:
Layer 1 β†’ [μ²΄ν¬ν¬μΈνΈ] β†’ Layer 2 β†’ Layer 3 β†’ [μ²΄ν¬ν¬μΈνΈ] β†’ ... β†’ Loss

Backward μ‹ μ²΄ν¬ν¬μΈνΈμ—μ„ μ¬κ³„μ‚°
β†’ λ©”λ¨λ¦¬: O(βL) - λ£¨νΈ λ μ΄μ–΄ μ
β†’ κ³„μ‚°: ~33% μ¦κ°€ (μ¬κ³„μ‚° λΉ„μ©)
```

### 4.2 κµ¬ν„

```python
import torch
from torch.utils.checkpoint import checkpoint, checkpoint_sequential

class TransformerBlockWithCheckpoint(nn.Module):
    """Checkpointingμ΄ μ μ©λ Transformer λΈ”λ΅"""

    def __init__(self, config, use_checkpoint=True):
        super().__init__()
        self.use_checkpoint = use_checkpoint

        self.attention = MultiHeadAttention(config)
        self.ffn = FeedForward(config)
        self.norm1 = nn.LayerNorm(config.hidden_size)
        self.norm2 = nn.LayerNorm(config.hidden_size)

    def forward(self, x, attention_mask=None):
        if self.use_checkpoint and self.training:
            # Checkpointing μ‚¬μ©
            return checkpoint(
                self._forward_impl,
                x, attention_mask,
                use_reentrant=False,  # PyTorch 2.0+ κ¶μ¥
            )
        else:
            return self._forward_impl(x, attention_mask)

    def _forward_impl(self, x, attention_mask):
        # Attention
        residual = x
        x = self.norm1(x)
        x = self.attention(x, attention_mask)
        x = residual + x

        # FFN
        residual = x
        x = self.norm2(x)
        x = self.ffn(x)
        x = residual + x

        return x


class TransformerWithSelectiveCheckpoint(nn.Module):
    """μ„ νƒμ  Checkpointing"""

    def __init__(self, config, checkpoint_ratio=0.5):
        super().__init__()
        self.layers = nn.ModuleList([
            TransformerBlockWithCheckpoint(
                config,
                # μΌλ¶€ λ μ΄μ–΄λ§ checkpoint
                use_checkpoint=(i % int(1/checkpoint_ratio) == 0)
            )
            for i in range(config.num_layers)
        ])

    def forward(self, x, attention_mask=None):
        for layer in self.layers:
            x = layer(x, attention_mask)
        return x
```

---

## 5. ν•™μµ μ•μ •μ„±

### 5.1 Loss Spike λ€μ‘

```python
class TrainingStabilizer:
    """ν•™μµ μ•μ •μ„± κ΄€λ¦¬"""

    def __init__(
        self,
        loss_spike_threshold: float = 5.0,  # μ΄μ „ λ€λΉ„ 5λ°°
        grad_norm_threshold: float = 10.0,
        window_size: int = 100
    ):
        self.loss_spike_threshold = loss_spike_threshold
        self.grad_norm_threshold = grad_norm_threshold
        self.window_size = window_size

        self.loss_history = []
        self.grad_norm_history = []
        self.skipped_steps = 0

    def check_loss_spike(self, loss: float) -> bool:
        """Loss spike κ°μ§€"""
        if len(self.loss_history) < self.window_size:
            self.loss_history.append(loss)
            return False

        avg_loss = sum(self.loss_history[-self.window_size:]) / self.window_size

        if loss > avg_loss * self.loss_spike_threshold:
            print(f"β οΈ Loss spike detected: {loss:.4f} (avg: {avg_loss:.4f})")
            return True

        self.loss_history.append(loss)
        return False

    def check_grad_norm(self, model: nn.Module) -> tuple[float, bool]:
        """Gradient norm μ²΄ν¬"""
        total_norm = 0.0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        total_norm = total_norm ** 0.5

        is_spike = total_norm > self.grad_norm_threshold

        if is_spike:
            print(f"β οΈ Gradient spike: {total_norm:.4f}")

        self.grad_norm_history.append(total_norm)
        return total_norm, is_spike

    def should_skip_step(self, loss: float, model: nn.Module) -> bool:
        """ν•΄λ‹Ή stepμ„ κ±΄λ„λ›Έμ§€ κ²°μ •"""
        loss_spike = self.check_loss_spike(loss)
        _, grad_spike = self.check_grad_norm(model)

        if loss_spike or grad_spike:
            self.skipped_steps += 1
            return True

        return False


def stable_training_step(
    model, batch, optimizer, stabilizer, scaler=None
):
    """μ•μ •μ μΈ ν•™μµ μ¤ν…"""

    # Forward
    with torch.cuda.amp.autocast():
        outputs = model(**batch)
        loss = outputs.loss

    # Loss spike μ²΄ν¬
    if stabilizer.should_skip_step(loss.item(), model):
        optimizer.zero_grad()
        print(f"Skipping step (total skipped: {stabilizer.skipped_steps})")
        return None

    # Backward
    if scaler:
        scaler.scale(loss).backward()

        # Gradient clipping
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        scaler.step(optimizer)
        scaler.update()
    else:
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

    optimizer.zero_grad()

    return loss.item()
```

### 5.2 μ²΄ν¬ν¬μΈνΈ μ „λµ

```python
import os
import shutil
from datetime import datetime

class CheckpointManager:
    """μ²΄ν¬ν¬μΈνΈ κ΄€λ¦¬"""

    def __init__(
        self,
        save_dir: str,
        max_checkpoints: int = 5,
        save_interval_steps: int = 1000,
        save_interval_hours: float = 1.0
    ):
        self.save_dir = save_dir
        self.max_checkpoints = max_checkpoints
        self.save_interval_steps = save_interval_steps
        self.save_interval_hours = save_interval_hours

        self.last_save_time = datetime.now()
        self.checkpoints = []

        os.makedirs(save_dir, exist_ok=True)

    def should_save(self, step: int) -> bool:
        """μ²΄ν¬ν¬μΈνΈ μ €μ¥ μ—¬λ¶€ κ²°μ •"""
        # μ¤ν… κΈ°λ°
        if step % self.save_interval_steps == 0:
            return True

        # μ‹κ°„ κΈ°λ°
        elapsed = (datetime.now() - self.last_save_time).total_seconds() / 3600
        if elapsed >= self.save_interval_hours:
            return True

        return False

    def save(
        self,
        model,
        optimizer,
        scheduler,
        step: int,
        loss: float,
        **extra
    ):
        """μ²΄ν¬ν¬μΈνΈ μ €μ¥"""
        checkpoint_name = f"checkpoint-{step}"
        checkpoint_path = os.path.join(self.save_dir, checkpoint_name)

        # μ €μ¥
        state = {
            'step': step,
            'loss': loss,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            **extra
        }

        torch.save(state, checkpoint_path + ".pt")

        # λ©”νƒ€λ°μ΄ν„°
        self.checkpoints.append({
            'path': checkpoint_path,
            'step': step,
            'loss': loss,
            'time': datetime.now().isoformat()
        })

        self.last_save_time = datetime.now()

        # μ¤λλ μ²΄ν¬ν¬μΈνΈ μ‚­μ 
        self._cleanup()

        print(f"π’Ύ Saved checkpoint: {checkpoint_name}")

    def _cleanup(self):
        """μ¤λλ μ²΄ν¬ν¬μΈνΈ μ •λ¦¬"""
        while len(self.checkpoints) > self.max_checkpoints:
            oldest = self.checkpoints.pop(0)
            if os.path.exists(oldest['path'] + ".pt"):
                os.remove(oldest['path'] + ".pt")
                print(f"π—‘οΈ Removed old checkpoint: {oldest['path']}")

    def load_latest(self) -> dict:
        """μµμ‹  μ²΄ν¬ν¬μΈνΈ λ΅λ“"""
        if not self.checkpoints:
            # λ””λ ‰ν† λ¦¬μ—μ„ μ°ΎκΈ°
            files = sorted([
                f for f in os.listdir(self.save_dir)
                if f.startswith("checkpoint-") and f.endswith(".pt")
            ])

            if not files:
                return None

            latest = files[-1]
            return torch.load(os.path.join(self.save_dir, latest))

        return torch.load(self.checkpoints[-1]['path'] + ".pt")
```

---

## 6. ν•™μµλ¥  μ¤μΌ€μ¤„λ§

### 6.1 Warmup + Cosine Decay

```python
import math
from torch.optim.lr_scheduler import LambdaLR

def get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps: int,
    num_training_steps: int,
    min_lr_ratio: float = 0.1,
    num_cycles: float = 0.5
):
    """
    Warmup + Cosine Decay μ¤μΌ€μ¤„λ¬

    ν•™μµ μ΄κΈ°: Linear warmup (0 β†’ max_lr)
    μ΄ν›„: Cosine decay (max_lr β†’ min_lr)
    """

    def lr_lambda(current_step):
        # Warmup
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))

        # Cosine decay
        progress = float(current_step - num_warmup_steps) / float(
            max(1, num_training_steps - num_warmup_steps)
        )

        cosine_decay = 0.5 * (1.0 + math.cos(math.pi * num_cycles * 2.0 * progress))

        # min_lrκΉμ§€λ§ κ°μ†
        decayed = (1 - min_lr_ratio) * cosine_decay + min_lr_ratio

        return decayed

    return LambdaLR(optimizer, lr_lambda)


# WSD (Warmup-Stable-Decay) μ¤μΌ€μ¤„λ¬ (Llama 2)
def get_wsd_schedule(
    optimizer,
    num_warmup_steps: int,
    num_stable_steps: int,
    num_decay_steps: int,
    min_lr_ratio: float = 0.1
):
    """
    Warmup-Stable-Decay μ¤μΌ€μ¤„λ¬

    1. Warmup: 0 β†’ max_lr
    2. Stable: max_lr μ μ§€
    3. Decay: max_lr β†’ min_lr (cosine)
    """
    total_steps = num_warmup_steps + num_stable_steps + num_decay_steps

    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            # Warmup phase
            return float(current_step) / float(max(1, num_warmup_steps))

        elif current_step < num_warmup_steps + num_stable_steps:
            # Stable phase
            return 1.0

        else:
            # Decay phase
            decay_step = current_step - num_warmup_steps - num_stable_steps
            progress = float(decay_step) / float(max(1, num_decay_steps))
            cosine_decay = 0.5 * (1.0 + math.cos(math.pi * progress))
            return (1 - min_lr_ratio) * cosine_decay + min_lr_ratio

    return LambdaLR(optimizer, lr_lambda)
```

---

## 7. μ‹¤μµ: μ™„μ „ν• ν•™μµ μ¤ν¬λ¦½νΈ

```python
import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import wandb

def main():
    """μ™„μ „ν• λ¶„μ‚° ν•™μµ μ¤ν¬λ¦½νΈ"""

    # 1. λ¶„μ‚° μ΄κΈ°ν™”
    dist.init_process_group(backend="nccl")
    local_rank = int(os.environ["LOCAL_RANK"])
    world_size = dist.get_world_size()
    torch.cuda.set_device(local_rank)

    # Rank 0λ§ λ΅κΉ…
    is_main = local_rank == 0

    if is_main:
        wandb.init(project="foundation-model-training")

    # 2. μ„¤μ •
    config = {
        'hidden_size': 4096,
        'num_layers': 32,
        'num_heads': 32,
        'vocab_size': 50257,
        'max_seq_len': 2048,
        'batch_size': 4,  # per GPU
        'gradient_accumulation': 8,
        'learning_rate': 3e-4,
        'warmup_steps': 2000,
        'total_steps': 100000,
        'weight_decay': 0.1,
        'max_grad_norm': 1.0,
    }

    effective_batch = config['batch_size'] * config['gradient_accumulation'] * world_size
    print(f"Effective batch size: {effective_batch}")

    # 3. λ¨λΈ
    model = TransformerModel(config).cuda()

    # Activation checkpointing
    model.gradient_checkpointing_enable()

    # DDP λλ” FSDP
    model = DDP(model, device_ids=[local_rank])

    # 4. λ°μ΄ν„°
    dataset = PretrainingDataset(config)
    sampler = DistributedSampler(dataset, shuffle=True)
    dataloader = DataLoader(
        dataset,
        batch_size=config['batch_size'],
        sampler=sampler,
        num_workers=4,
        pin_memory=True,
    )

    # 5. Optimizer & Scheduler
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay'],
        betas=(0.9, 0.95),
    )

    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=config['warmup_steps'],
        num_training_steps=config['total_steps'],
    )

    # 6. μ ν‹Έλ¦¬ν‹°
    scaler = torch.cuda.amp.GradScaler()
    stabilizer = TrainingStabilizer()
    checkpoint_mgr = CheckpointManager("checkpoints")

    # μ²΄ν¬ν¬μΈνΈ λ³µμ›
    checkpoint = checkpoint_mgr.load_latest()
    start_step = 0
    if checkpoint:
        model.module.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if checkpoint['scheduler_state_dict']:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_step = checkpoint['step']
        if is_main:
            print(f"Resumed from step {start_step}")

    # 7. ν•™μµ λ£¨ν”„
    model.train()
    global_step = start_step
    accumulated_loss = 0.0

    for epoch in range(100):  # μ¶©λ¶„ν ν° μ
        sampler.set_epoch(epoch)

        for batch_idx, batch in enumerate(dataloader):
            batch = {k: v.cuda() for k, v in batch.items()}

            # Forward (Mixed Precision)
            with torch.cuda.amp.autocast(dtype=torch.bfloat16):
                outputs = model(**batch)
                loss = outputs.loss / config['gradient_accumulation']

            # Backward
            scaler.scale(loss).backward()
            accumulated_loss += loss.item()

            # Gradient Accumulation
            if (batch_idx + 1) % config['gradient_accumulation'] == 0:
                # Gradient clipping
                scaler.unscale_(optimizer)
                grad_norm = torch.nn.utils.clip_grad_norm_(
                    model.parameters(),
                    config['max_grad_norm']
                )

                # Step
                scaler.step(optimizer)
                scaler.update()
                scheduler.step()
                optimizer.zero_grad()

                global_step += 1

                # λ΅κΉ…
                if is_main and global_step % 10 == 0:
                    lr = scheduler.get_last_lr()[0]
                    wandb.log({
                        'loss': accumulated_loss,
                        'learning_rate': lr,
                        'grad_norm': grad_norm.item(),
                        'step': global_step,
                    })
                    print(f"Step {global_step}: loss={accumulated_loss:.4f}, lr={lr:.2e}")

                accumulated_loss = 0.0

                # μ²΄ν¬ν¬μΈνΈ
                if checkpoint_mgr.should_save(global_step):
                    if is_main:
                        checkpoint_mgr.save(
                            model.module, optimizer, scheduler,
                            global_step, accumulated_loss
                        )

                # μΆ…λ£ μ΅°κ±΄
                if global_step >= config['total_steps']:
                    break

        if global_step >= config['total_steps']:
            break

    # μ •λ¦¬
    dist.destroy_process_group()
    if is_main:
        wandb.finish()


if __name__ == "__main__":
    main()

# μ‹¤ν–‰:
# torchrun --nproc_per_node=8 --nnodes=4 --node_rank=0 \
#          --master_addr="master" --master_port=29500 train.py
```

---

## μ°Έκ³  μλ£

### λ¬Έμ„
- [PyTorch FSDP](https://pytorch.org/docs/stable/fsdp.html)
- [DeepSpeed](https://www.deepspeed.ai/)
- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)

### λ…Όλ¬Έ
- Rajbhandari et al. (2020). "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models"
- Narayanan et al. (2021). "Efficient Large-Scale Language Model Training on GPU Clusters"

### κ΄€λ ¨ λ μ¨
- [../Deep_Learning/11_Model_Deployment.md](../Deep_Learning/11_Model_Deployment.md)
- [../MLOps/08_Model_Serving_Basics.md](../MLOps/08_Model_Serving_Basics.md)

---

## μ—°μµ λ¬Έμ 

### μ—°μµ λ¬Έμ  1: λ©”λ¨λ¦¬ μμ‚° μ¶”μ •

νΈλμ¤ν¬λ¨Έ λ¨λΈμ κµ¬μ„±μ΄ λ‹¤μκ³Ό κ°™μµλ‹λ‹¤:
- νλΌλ―Έν„°: 7B (float16, κ° 2 λ°”μ΄νΈ)
- λ°°μΉ ν¬κΈ°: 4, μ‹ν€€μ¤ κΈΈμ΄: 2048
- νλ“  μ°¨μ›: 4096, 32 λ μ΄μ–΄
- AdamW μµν‹°λ§μ΄μ € μ‚¬μ© (fp32 μµν‹°λ§μ΄μ € μƒνƒ)

λ‹¤μ κ° ν•­λ©μ— ν•„μ”ν• GPU λ©”λ¨λ¦¬λ¥Ό μ¶”μ •ν•μ„Έμ”:
1. λ¨λΈ νλΌλ―Έν„°
2. μµν‹°λ§μ΄μ € μƒνƒ (AdamW: νλΌλ―Έν„° λ³µμ‚¬λ³Έ + 1μ°¨ λ¨λ©νΈ + 2μ°¨ λ¨λ©νΈ, λ¨λ‘ fp32)
3. κ·Έλλ””μ–ΈνΈ (fp32)

μ–΄λ ν•­λ©μ΄ λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ„ μ§€λ°°ν•λ‚μ”?

<details>
<summary>μ •λ‹µ λ³΄κΈ°</summary>

**1. λ¨λΈ νλΌλ―Έν„° (fp16):**
```
7B νλΌλ―Έν„° Γ— 2 λ°”μ΄νΈ = 14 GB
```

**2. μµν‹°λ§μ΄μ € μƒνƒ (AdamW, fp32):**
AdamWλ” νλΌλ―Έν„°λ§λ‹¤ μ„Έ κ°€μ§€ fp32 λ³µμ‚¬λ³Έμ„ μ μ§€ν•©λ‹λ‹¤:
- fp32 λ§μ¤ν„° νλΌλ―Έν„° λ³µμ‚¬λ³Έ: 7B Γ— 4 λ°”μ΄νΈ = 28 GB
- 1μ°¨ λ¨λ©νΈ: 7B Γ— 4 λ°”μ΄νΈ = 28 GB
- 2μ°¨ λ¨λ©νΈ: 7B Γ— 4 λ°”μ΄νΈ = 28 GB
ν•©κ³„: **84 GB**

**3. κ·Έλλ””μ–ΈνΈ (fp32):**
```
7B Γ— 4 λ°”μ΄νΈ = 28 GB
```

**ν•©κ³„ (ν™μ„±ν™” μ μ™Έ):** 14 + 84 + 28 = **~126 GB**

**μ§€λ°°μ μΈ ν•­λ©:** μµν‹°λ§μ΄μ € μƒνƒ(84 GB)κ°€ λ‹¨μ—° κ°€μ¥ ν° ν•­λ©μ…λ‹λ‹¤ β€” λ¨λΈ νλΌλ―Έν„° μμ²΄λ³΄λ‹¤ 6λ°° λ” ν½λ‹λ‹¤. μ΄κ²ƒμ΄ ZeRO Stage 1μ΄ μµν‹°λ§μ΄μ € μƒνƒ μƒ¤λ”©μ„ λ¨Όμ € νƒ€κ²μΌλ΅ ν•λ” μ΄μ μ…λ‹λ‹¤: μ§€λ°°μ μΈ λΉ„μ©μ„ μ¦‰μ‹ μ κ°ν•©λ‹λ‹¤.

μ°Έκ³ : ν™μ„±ν™”(Activation)λ” batch_size Γ— seq_len Γ— hidden_dim Γ— num_layersμ— λΉ„λ΅€ν•λ” μ¶”κ°€ λ©”λ¨λ¦¬λ¥Ό μ‚¬μ©ν•λ©°, μ΄ κµ¬μ„±μ—μ„λ” μ•½ 4 Γ— 2048 Γ— 4096 Γ— 32 Γ— 2 λ°”μ΄νΈ β‰ 2 GB β€” μµν‹°λ§μ΄μ € μƒνƒμ— λΉ„ν•΄ μƒλ€μ μΌλ΅ μ‘μµλ‹λ‹¤.

</details>

---

### μ—°μµ λ¬Έμ  2: λ³‘λ ¬ν™” μ „λµ μ„ νƒ

κ° ν•™μµ μ‹λ‚λ¦¬μ¤μ— λ€ν•΄ κ°€μ¥ μ ν•©ν• λ³‘λ ¬ν™” μ „λµ(λλ” μ΅°ν•©)μ„ μ¶”μ²ν•κ³  μ΄μ λ¥Ό μ„¤λ…ν•μ„Έμ”.

1. **μ‹λ‚λ¦¬μ¤ A**: 7B νλΌλ―Έν„° λ¨λΈ, λ‹¨μΌ λ…Έλ“μ 8Γ— A100 80GB GPU. λ¨λΈμ΄ fp16μ—μ„ λ‹¨μΌ GPUμ— λ§μ.
2. **μ‹λ‚λ¦¬μ¤ B**: 70B νλΌλ―Έν„° λ¨λΈ, 8κ° λ…Έλ“μ— κ±Έμ³ 64Γ— A100 80GB GPU. λ¨λΈμ΄ fp16μ—μ„ λ‹¨μΌ GPUμ— λ§μ§€ μ•μ.
3. **μ‹λ‚λ¦¬μ¤ C**: 7B λ¨λΈ, 1024 ν† ν° μ‹ν€€μ¤μ§€λ§, 8K μ»¨ν…μ¤νΈ κΈΈμ΄μ—μ„ μ–΄ν…μ… κ³„μ‚°μ΄ λ³‘λ©.

<details>
<summary>μ •λ‹µ λ³΄κΈ°</summary>

**μ‹λ‚λ¦¬μ¤ A: 7B, 8 GPU, λ‹¨μΌ λ…Έλ“, λ‹¨μΌ GPUμ— λ§μ**
- **κ¶μ¥: λ°μ΄ν„° λ³‘λ ¬ν™”(DDP λλ” FSDP)**
- λ¨λΈμ΄ λ‹¨μΌ GPUμ— λ§μΌλ―€λ΅ λ¨λΈ μμ²΄λ¥Ό λ¶„ν• ν•  ν•„μ”κ°€ μ—†μµλ‹λ‹¤. λ°μ΄ν„° λ³‘λ ¬ν™”λ” λ‹¨μν λ¨λΈμ„ λ³µμ ν•κ³  λ°°μΉλ¥Ό 8κ° GPUμ— λ¶„μ‚°ν•©λ‹λ‹¤.
- FSDP(ZeRO-3)λ¥Ό μ‚¬μ©ν•λ©΄ λ¨λΈμ΄ λ©”λ¨λ¦¬μ— λ§λ”λΌλ„ μµν‹°λ§μ΄μ € μƒνƒλ¥Ό μƒ¤λ”©ν•μ—¬ GPUλ‹Ή λ©”λ¨λ¦¬λ¥Ό ~4λ°° μ¤„μ΄κ³  λ” ν° λ°°μΉ ν¬κΈ°λ¥Ό ν—μ©ν•©λ‹λ‹¤.
- λ…Έλ“ λ‚΄ NVLink λ€μ—­ν­μΌλ΅ all-reduceκ°€ λ§¤μ° λΉ λ¦…λ‹λ‹¤.

**μ‹λ‚λ¦¬μ¤ B: 70B, 64 GPU, 8 λ…Έλ“, λ‹¨μΌ GPUμ— λ§μ§€ μ•μ**
- **κ¶μ¥: 3D λ³‘λ ¬ν™” (TP + PP + DP)**
- **ν…μ„ λ³‘λ ¬ν™”(TP=4)**: λ…Έλ“ λ‚΄μ—μ„ κ° νΈλμ¤ν¬λ¨Έ λ μ΄μ–΄λ¥Ό 4κ° GPUμ— λ¶„ν• (λΉ λ¥Έ NVLink ν™μ©).
- **νμ΄ν”„λΌμΈ λ³‘λ ¬ν™”(PP=2)**: 32+ λ μ΄μ–΄λ¥Ό 2κ° νμ΄ν”„λΌμΈ μ¤ν…μ΄μ§€λ΅ λ¶„ν• ν•μ—¬ λ…Έλ“ κ°„ λ°°λ¶„(λ” λλ¦° λ…Έλ“ κ°„ λ€μ—­ν­ ν—μ©).
- **λ°μ΄ν„° λ³‘λ ¬ν™”(DP=8)**: TPΓ—PP "λ¨λΈ λ³µμ λ³Έ"μ„ 8κ° λ³µμ‚¬λ³ΈμΌλ΅ λ³µμ .
- μ΄ GPU: 4 Γ— 2 Γ— 8 = 64 β“

**μ‹λ‚λ¦¬μ¤ C: 7B, 8K μ»¨ν…μ¤νΈ, μ–΄ν…μ… λ³‘λ©**
- **κ¶μ¥: μ‹ν€€μ¤ λ³‘λ ¬ν™”(SP) + FlashAttention**
- 8K μ»¨ν…μ¤νΈμ—μ„ ν‘μ¤€ μ–΄ν…μ…μ€ λ μ΄μ–΄λ‹Ή O(8KΒ²) = 6400λ§ μ—°μ‚° β€” λλ¦¬κ³  λ©”λ¨λ¦¬ μ§‘μ•½μ μ…λ‹λ‹¤.
- **μ‹ν€€μ¤ λ³‘λ ¬ν™”(Sequence Parallelism)**λ” μ‹ν€€μ¤ μ°¨μ›μ„ GPUμ— λ¶„μ‚°ν•μ—¬ μ–΄ν…μ… κ³„μ‚° λΉ„μ©μ„ λ¶„μ‚°ν•©λ‹λ‹¤.
- **FlashAttention**μ€ μ „μ²΄ μ–΄ν…μ… ν–‰λ ¬μ„ κµ¬μ²΄ν™”ν•μ§€ μ•κ³  νƒ€μΌ(tile) λ‹¨μ„λ΅ κ³„μ‚°ν•μ—¬ μ–΄ν…μ…μ λ©”λ¨λ¦¬ κ³µκ°„μ„ O(nΒ²)μ—μ„ O(n)μΌλ΅ μ¤„μ…λ‹λ‹¤.
- μ΄ λ‘ κΈ°λ²•μ„ κ²°ν•©ν•  μ μμµλ‹λ‹¤.

</details>

---

### μ—°μµ λ¬Έμ  3: κ·Έλλ””μ–ΈνΈ ν΄λ¦¬ν•‘(Gradient Clipping) λ¶„μ„

μμ—…μ—μ„ κ·Έλλ””μ–ΈνΈ ν΄λ¦¬ν•‘μ„ λ³΄μ—¬μ¤λ‹λ‹¤: `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)`.

1. μ „μ—­ κ·Έλλ””μ–ΈνΈ λ…Έλ¦„(global gradient norm) ν΄λ¦¬ν•‘μ΄ μν•™μ μΌλ΅ λ¬΄μ—‡μ„ ν•λ”μ§€ μ„¤λ…ν•μ„Έμ”.
2. μ™ κ·Έλλ””μ–ΈνΈ ν΄λ¦¬ν•‘μ΄ ν•™μµ μ•μ •μ„±μ— μ¤‘μ”ν•κ°€μ”, νΉν λ€ν• λ¨λΈ ν•™μµμ μ΄κΈ° λ‹¨κ³„μ—μ„?
3. `max_norm`μ„ λ„λ¬΄ μ‘κ² μ„¤μ •ν•λ©΄(μ: 0.001) λλ” λ„λ¬΄ ν¬κ² μ„¤μ •ν•λ©΄(μ: 1000) μ–΄λ–»κ² λλ‚μ”?

<details>
<summary>μ •λ‹µ λ³΄κΈ°</summary>

**1. κ·Έλλ””μ–ΈνΈ λ…Έλ¦„ ν΄λ¦¬ν•‘μ μν–‰ λ‚΄μ©:**

λ‹¨μΌ λ²΅ν„°λ΅ μ—°κ²°λ λ¨λ“  κ·Έλλ””μ–ΈνΈμ μ „μ—­ L2 λ…Έλ¦„μ„ κ³„μ‚°ν•©λ‹λ‹¤:
```
global_norm = sqrt(sum(g_iΒ² for all parameter gradients g_i))
```
`global_norm > max_norm`μ΄λ©΄, λ¨λ“  κ·Έλλ””μ–ΈνΈλ¥Ό `max_norm / global_norm`μΌλ΅ μ¤μΌ€μΌλ§ν•©λ‹λ‹¤:
```
g_i β† g_i Γ— (max_norm / global_norm)
```
μ΄λ¥Ό ν†µν•΄ κ·Έλλ””μ–ΈνΈ μ—…λ°μ΄νΈμ λ°©ν–¥μ€ μ μ§€ν•λ©΄μ„ ν¬κΈ°λ¥Ό μ ν•ν•©λ‹λ‹¤.

**2. λ€ν• λ¨λΈ ν•™μµ μ•μ •μ„±μ— μ¤‘μ”ν• μ΄μ :**

- λ€ν• λ¨λΈμ€ λ§μ€ λ μ΄μ–΄λ¥Ό κ°€μ§€λ©°, κ·Έλλ””μ–ΈνΈλ” λ μ΄μ–΄λ¥Ό ν†µν•΄ κ³±μ…μ μΌλ΅ μ¦ν­λ  μ μμµλ‹λ‹¤(κ·Έλλ””μ–ΈνΈ ν­λ°).
- ν•™μµ μ΄κΈ°μ—λ” νλΌλ―Έν„° μ΄κΈ°ν™”κ°€ μ†μ‹¤μ΄ μ•μ •λκΈ° μ „μ— λ§¤μ° ν° κ·Έλλ””μ–ΈνΈ ν¬κΈ°λ¥Ό μƒμ„±ν•  μ μμµλ‹λ‹¤.
- λ‹¨μΌ λ€ν• κ·Έλλ””μ–ΈνΈ μ¤ν…μ΄ μ΄μ „μ— ν•™μµλ ν‘ν„μ„ "νκ΄΄"ν•μ—¬ λ§μ€ μ¤ν…μ΄ λ³µκµ¬μ— ν•„μ”ν•κ² λ©λ‹λ‹¤.
- μ†μ‹¤ μ¤νμ΄ν¬(Loss spike) β€” μ†μ‹¤μ κ°‘μ‘μ¤λ¬μ΄ μƒμΉ β€” μ€ μΆ…μΆ… μ΄λ° λ€ν• μ—…λ°μ΄νΈλ΅ μΈν•΄ λ°μƒν•©λ‹λ‹¤. κ·Έλλ””μ–ΈνΈ ν΄λ¦¬ν•‘μ€ μ—…λ°μ΄νΈ ν¬κΈ°μ— μƒν•μ„ μ„ μ κ³µν•©λ‹λ‹¤.

**3. max_norm λ„λ¬΄ μ‘μ vs λ„λ¬΄ νΌ:**

- **max_norm = 0.001 (λ„λ¬΄ μ‘μ)**: κ±°μ λ¨λ“  κ·Έλλ””μ–ΈνΈ μ—…λ°μ΄νΈκ°€ μ‹¬ν•κ² μ¤μΌ€μΌ λ‹¤μ΄λ©λ‹λ‹¤. ν•™μµμ΄ κ·Ήλ„λ΅ λλ ¤μ§‘λ‹λ‹¤ β€” λ¨λΈμ΄ νλΌλ―Έν„° κ³µκ°„μ—μ„ κ±°μ μ΄λ™ν•μ§€ μ•μµλ‹λ‹¤. κ²°κµ­ λ¨λΈμ΄ μλ ΄ν•  μ μμ§€λ§ ν›¨μ”¬ λ” λ§μ€ μ¤ν…μ΄ ν•„μ”ν•©λ‹λ‹¤. μ΄λ” λ§¤μ° μ‘μ€ μ ν¨ ν•™μµλ¥ μ„ μ‚¬μ©ν•λ” κ²ƒκ³Ό λ™μΌν•©λ‹λ‹¤.
- **max_norm = 1000 (λ„λ¬΄ νΌ)**: ν΄λ¦¬ν•‘μ΄ μ‚¬μ‹¤μƒ ν™μ„±ν™”λμ§€ μ•μµλ‹λ‹¤(λ€λ¶€λ¶„μ κ·Έλλ””μ–ΈνΈ λ…Έλ¦„μ€ 1000 μ΄ν•). λ¨λΈμ€ ν΄λ¦¬ν•‘μ΄ μ—†λ” κ²ƒμ²λΌ ν•™μµλ©λ‹λ‹¤ β€” μ •μƒ ν•™μµ μ¤‘μ—λ” κ΄μ°®μ§€λ§ λ¶μ•μ •ν• κΈ°κ°„ λ™μ• κ·Έλλ””μ–ΈνΈ ν­λ°λ΅λ¶€ν„° λ³΄νΈλ°›μ§€ λ»ν•©λ‹λ‹¤.

μ‹¤μ λ΅ max_norm = 1.0μ€ λ€ν• μ–Έμ–΄ λ¨λΈμ μΌλ°μ μΈ κΈ°λ³Έκ°’μ΄λ©°, μ΄λ” λ¶μ•μ •μ„± λ°μƒ μ‹μ—λ§ λ“λ¬Όκ² ν™μ„±ν™”λμ–΄μ•Ό ν•©λ‹λ‹¤.

</details>

---

### μ—°μµ λ¬Έμ  4: ZeRO μ¤ν…μ΄μ§€ λΉ„κµ

DeepSpeedμ ZeRO μµν‹°λ§μ΄μ €μ—λ” μ„Έ κ°€μ§€ μ¤ν…μ΄μ§€κ°€ μμµλ‹λ‹¤. μ•„λ ν‘λ¥Ό μ™„μ„±ν•μ„Έμ”:

| μ¤ν…μ΄μ§€ | μƒ¤λ”© λ€μƒ | λ©”λ¨λ¦¬ μ κ° (κ·Όμ‚¬μΉ) | ν†µμ‹  μ¤λ²„ν—¤λ“ |
|---------|---------|-----------------|------------|
| ZeRO-1 | ? | ? | ? |
| ZeRO-2 | ? | ? | ? |
| ZeRO-3 | ? | ? | ? |

<details>
<summary>μ •λ‹µ λ³΄κΈ°</summary>

| μ¤ν…μ΄μ§€ | μƒ¤λ”© λ€μƒ | λ©”λ¨λ¦¬ μ κ° (κ·Όμ‚¬μΉ) | ν†µμ‹  μ¤λ²„ν—¤λ“ |
|---------|---------|-----------------|------------|
| **ZeRO-1** | Nκ° GPUμ— μµν‹°λ§μ΄μ € μƒνƒ(λ¨λ©ν…€ + λ¶„μ‚°) λ¶„μ‚° | μµν‹°λ§μ΄μ € μƒνƒ λ©”λ¨λ¦¬ μ•½ 4λ°° κ°μ† | μµμ†: νλΌλ―Έν„° μ—…λ°μ΄νΈ ν›„ all-reduceλ§ (DDPμ™€ λ™μΌ) |
| **ZeRO-2** | Nκ° GPUμ— μµν‹°λ§μ΄μ € μƒνƒ + κ·Έλλ””μ–ΈνΈ λ¶„μ‚° | μµν‹°λ§μ΄μ € + κ·Έλλ””μ–ΈνΈ λ©”λ¨λ¦¬ μ•½ 8λ°° κ°μ† | λ³΄ν†µ: all-reduce λ€μ‹  reduce-scatter κ·Έλλ””μ–ΈνΈ (DDPλ³΄λ‹¤ μ•½κ°„ λ” ν¨μ¨μ ) |
| **ZeRO-3** | Nκ° GPUμ— μµν‹°λ§μ΄μ € μƒνƒ + κ·Έλλ””μ–ΈνΈ + λ¨λΈ νλΌλ―Έν„° λ¶„μ‚° | ~Nλ°° κ°μ† (GPU μμ— μ„ ν• λΉ„λ΅€) | μƒλ‹Ήν•¨: κ° ν¬μ›λ“/λ°±μ›λ“ ν¨μ¤ μ „μ— νλΌλ―Έν„° all-gather ν•„μ”; λ μ΄μ–΄λ‹Ή ν†µμ‹  μ§€μ—° μ¶”κ°€ |

**ν•µμ‹¬ μΈμ‚¬μ΄νΈ:** ZeRO-3λ” μµκ³ μ λ©”λ¨λ¦¬ ν¨μ¨μ„±μ„ λ‹¬μ„±ν•©λ‹λ‹¤(λ‹¨μΌ GPU μ©λ‰μ Nλ°° λ” ν° λ¨λΈ ν•™μµ κ°€λ¥), ν•μ§€λ§ ν‘μ¤€ DDP λ€λΉ„ ν•™μµ μ¤ν…λ‹Ή 2λ°° λ” λ§μ€ ν†µμ‹  μ—°μ‚°μ λΉ„μ©μ΄ λ°μƒν•©λ‹λ‹¤. λ¨λΈ ν¬κΈ°κ°€ λ‹¨μΌ GPU μ©λ‰μ„ μ΄κ³Όν•  λ• ν†µμ‹  μ¤λ²„ν—¤λ“λ” κ°€μΉκ°€ μμ§€λ§, λ©”λ¨λ¦¬μ— λ§λ” λ¨λΈμ κ²½μ° λ‚®μ€ μ§€μ—° μ‹κ°„μ„ μ„ν•΄ ZeRO-1 λλ” ZeRO-2κ°€ μ„ νΈλ©λ‹λ‹¤.

</details>
